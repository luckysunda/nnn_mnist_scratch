{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#-----------------one hot encoding of labels----------------------\n",
    "def label_encoder(y):\n",
    "    one_hot_y=[]\n",
    "    y=y.T\n",
    "    for d in y:\n",
    "        n=np.zeros(10)\n",
    "        n[d]=1\n",
    "        one_hot_y.append(n)\n",
    "    return np.array(one_hot_y).T\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class sigmoid_layer():\n",
    "    def __init__(self, n):\n",
    "        self.n=n\n",
    "        self.weight=np.random.randn(self.n,x.shape[0])\n",
    "        self.bias=np.random.randn(self.n,1)\n",
    "        \n",
    "    def der():\n",
    "        pass\n",
    "        \n",
    "    def forward(self, x):\n",
    "        self.z= np.dot(self.weight,x)+self.bias\n",
    "        self.a=1/(1+np.exp(x))\n",
    "        return self.a\n",
    "    \n",
    "        \n",
    "    def backward(self, delta_w):\n",
    "        delta=np.dot(delta_w,self.der(self.z))   \n",
    "        dw=np.dot(delta, self.x)\n",
    "        db=delta\n",
    "        self.weight-=0.1*dw\n",
    "        self.bias-=0.1*db\n",
    "        return np.dot(delta,self.weight)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class loss():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def mse_loss_function(y_true, y_pred):\n",
    "        n = y_pred.shape[1]\n",
    "        cost = (1./(2*n)) * np.sum((y_true - y_pred) ** 2)\n",
    "        return cost\n",
    "\n",
    "    def mse_loss_derivative(y_true, y_pred):\n",
    "        cost_prime = y_pred - y_true\n",
    "        return cost_prime\n",
    "\n",
    "    def cross_entropy_loss(y_pred,y_true):\n",
    "        y_true=label_encoder(y_true)\n",
    "        loss=[]\n",
    "        for i in range(y_true.shape[1]):\n",
    "            loss.append(-np.dot(y_true[:,i],np.log(1e-15+y_pred[:,i])))\n",
    "        return loss\n",
    "\n",
    "    def cross_entropy_derivative(y_pred, y_true):    \n",
    "        y_true=label_encoder(y_true)\n",
    "        return (y_pred-y_true)/y_pred.shape[1]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class softmax_layer():\n",
    "    def __init__(self,n):\n",
    "        self.lr=0.01\n",
    "        self.weight=np.random.randn(n[0],n[1])* np.sqrt(1 / n[1])\n",
    "        self.bias=np.random.rand(n[0],1)\n",
    "        self.old_w=self.weight\n",
    "    \n",
    "    def forward(self,x):\n",
    "        self.old_w=self.weight\n",
    "        self.x=x\n",
    "        self.z= np.dot(self.weight,self.x)+self.bias \n",
    "        arr=[]\n",
    "        for i in range(self.x.shape[1]):\n",
    "            exps=np.exp(self.z[:,i]-np.max(self.z[:,i]))\n",
    "            sfx=exps/np.sum(exps)\n",
    "            arr.append(sfx)\n",
    "        self.a=np.array(arr).T\n",
    "        return self.a\n",
    "\n",
    "    \n",
    "    def backward(self, delta_w):\n",
    "        delta=delta_w  \n",
    "        dw=np.dot(delta, self.x.T)\n",
    "        db=np.expand_dims(delta.mean(axis=1),1)\n",
    "        self.weight-=self.lr*dw\n",
    "        self.bias-=self.lr*db\n",
    "        return np.dot( self.old_w.T, delta)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class relu_layer:\n",
    "    def __init__(self, n):\n",
    "        \n",
    "        self.lr=0.01\n",
    "        self.weight=np.random.randn(n[0],n[1])* np.sqrt(1 / n[1])\n",
    "        self.bias=np.random.rand(n[0],1)\n",
    "        self.old_w=self.weight\n",
    "        \n",
    "    def der(self,x):\n",
    "        x[x>0]=1\n",
    "        x[x<=0]=0\n",
    "        return x\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        self.old_w=self.weight\n",
    "        self.x=x\n",
    "        self.z= np.dot(self.weight,self.x)+self.bias\n",
    "        self.a=np.maximum(0,self.z)\n",
    "        return self.a\n",
    "    \n",
    "    def backward(self, delta_w):\n",
    "        delta=delta_w*self.der(self.z) \n",
    "        dw=np.dot(delta, self.x.T)\n",
    "        db=np.expand_dims(delta.mean(axis=1),1)\n",
    "        self.weight-=self.lr*dw\n",
    "        self.bias-=self.lr*db\n",
    "        return np.dot(self.old_w.T, delta)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class neural_network:\n",
    "    def __init__(self):\n",
    "        self.r1=relu_layer([300, 784])\n",
    "        self.r2=relu_layer([100,300])\n",
    "        self.r3=relu_layer([200,100])\n",
    "        self.s1=softmax_layer([10,200])\n",
    "        \n",
    "    def forward(self, a0): \n",
    "        a1=self.r1.forward(a0)\n",
    "        a2=self.r2.forward(a1)\n",
    "        a3=self.r3.forward(a2)\n",
    "        out=self.s1.forward(a3)\n",
    "        return out\n",
    "    \n",
    "    def backward(self,y_pred,y_true):\n",
    "        delta_L=loss.cross_entropy_derivative(y_pred,y_true)\n",
    "        delta_4=self.s1.backward(delta_L)\n",
    "        delta_3=self.r3.backward(delta_4)\n",
    "        delta_2=self.r2.backward(delta_3)\n",
    "        delta_1=self.r1.backward(delta_2)\n",
    "  \n",
    "        \n",
    "    def train(self, x, y, batch_size, epochs):\n",
    "        for e in range(epochs):\n",
    "            if x.shape[1]%batch_size==0:\n",
    "                n_batches=x.shape[1]//batch_size\n",
    "            else:\n",
    "                n_batches=x.shape[1]//batch_size-1\n",
    "            print(\"n_batches\", n_batches)\n",
    "                \n",
    "            batches_x=[x[:,batch_size*i:batch_size*(i+1)] for i in range(n_batches)]\n",
    "            batches_y=[y[:,batch_size*i:batch_size*(i+1)] for i in range(n_batches)]\n",
    "            \n",
    "            \n",
    "            for bx,by in zip(batches_x, batches_y):\n",
    "                out=self.forward(bx)\n",
    "                self.backward(out,by)\n",
    "                ans=[]\n",
    "                for i in range(out.shape[1]):\n",
    "                    ans.append(np.argmax(out[:,i]))\n",
    "                l=loss.cross_entropy_loss(out,by)\n",
    "                a=accuracy_score(ans,by.squeeze(axis=0))\n",
    "                print(\"epoch: {}, loss: {}, acc: {}\".format(e,np.mean(l), a))\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_batches 24\n",
      "epoch: 0, loss: 2.8210179121058565, acc: 0.1035\n",
      "epoch: 0, loss: 2.602667249316266, acc: 0.1005\n",
      "epoch: 0, loss: 2.45042050016387, acc: 0.107\n",
      "epoch: 0, loss: 2.3738348242214, acc: 0.109\n",
      "epoch: 0, loss: 2.331977534380676, acc: 0.121\n",
      "epoch: 0, loss: 2.3057588112306737, acc: 0.162\n",
      "epoch: 0, loss: 2.291570095704979, acc: 0.1635\n",
      "epoch: 0, loss: 2.260819690872537, acc: 0.191\n",
      "epoch: 0, loss: 2.2555882024662615, acc: 0.2005\n",
      "epoch: 0, loss: 2.232253020982335, acc: 0.245\n",
      "epoch: 0, loss: 2.229654682907394, acc: 0.275\n",
      "epoch: 0, loss: 2.2194485014486824, acc: 0.288\n",
      "epoch: 0, loss: 2.214072453612866, acc: 0.2755\n",
      "epoch: 0, loss: 2.2079664288902374, acc: 0.303\n",
      "epoch: 0, loss: 2.2051078194676377, acc: 0.2735\n",
      "epoch: 0, loss: 2.184355282319266, acc: 0.32\n",
      "epoch: 0, loss: 2.1732236285635955, acc: 0.324\n",
      "epoch: 0, loss: 2.177528943404593, acc: 0.309\n",
      "epoch: 0, loss: 2.160744621241282, acc: 0.3515\n",
      "epoch: 0, loss: 2.1620103836389206, acc: 0.3455\n",
      "epoch: 0, loss: 2.152247707218483, acc: 0.3645\n",
      "epoch: 0, loss: 2.151818027986899, acc: 0.3685\n",
      "epoch: 0, loss: 2.1475515231038553, acc: 0.364\n",
      "epoch: 0, loss: 2.1356216488374353, acc: 0.3905\n",
      "n_batches 24\n",
      "epoch: 1, loss: 2.1128577013925534, acc: 0.4365\n",
      "epoch: 1, loss: 2.124869299408503, acc: 0.4025\n",
      "epoch: 1, loss: 2.1106880431934307, acc: 0.4325\n",
      "epoch: 1, loss: 2.100006519619171, acc: 0.448\n",
      "epoch: 1, loss: 2.085641402136073, acc: 0.4695\n",
      "epoch: 1, loss: 2.0860135055951594, acc: 0.4705\n",
      "epoch: 1, loss: 2.0864224844915644, acc: 0.463\n",
      "epoch: 1, loss: 2.0697508447662596, acc: 0.487\n",
      "epoch: 1, loss: 2.064068516913529, acc: 0.4805\n",
      "epoch: 1, loss: 2.0481656753521484, acc: 0.497\n",
      "epoch: 1, loss: 2.0465780556906017, acc: 0.489\n",
      "epoch: 1, loss: 2.0272159027087793, acc: 0.538\n",
      "epoch: 1, loss: 2.036147002364284, acc: 0.497\n",
      "epoch: 1, loss: 2.0266155220626025, acc: 0.518\n",
      "epoch: 1, loss: 2.029266170896705, acc: 0.485\n",
      "epoch: 1, loss: 1.9999848152600184, acc: 0.5275\n",
      "epoch: 1, loss: 1.9866074374725993, acc: 0.533\n",
      "epoch: 1, loss: 1.9920055775746834, acc: 0.5165\n",
      "epoch: 1, loss: 1.9723553354839527, acc: 0.543\n",
      "epoch: 1, loss: 1.9787310884734817, acc: 0.5275\n",
      "epoch: 1, loss: 1.9698506761096823, acc: 0.5275\n",
      "epoch: 1, loss: 1.9677249607736607, acc: 0.534\n",
      "epoch: 1, loss: 1.9673800233944572, acc: 0.5225\n",
      "epoch: 1, loss: 1.950139542412593, acc: 0.5565\n",
      "n_batches 24\n",
      "epoch: 2, loss: 1.9238731069959605, acc: 0.582\n",
      "epoch: 2, loss: 1.9380064438303337, acc: 0.5595\n",
      "epoch: 2, loss: 1.9245411555198295, acc: 0.564\n",
      "epoch: 2, loss: 1.9124327089293693, acc: 0.585\n",
      "epoch: 2, loss: 1.8964776218482968, acc: 0.5895\n",
      "epoch: 2, loss: 1.8992390622111535, acc: 0.595\n",
      "epoch: 2, loss: 1.9018634439075364, acc: 0.583\n",
      "epoch: 2, loss: 1.8802570506501488, acc: 0.5925\n",
      "epoch: 2, loss: 1.8759965303875064, acc: 0.5905\n",
      "epoch: 2, loss: 1.8595056883665149, acc: 0.5975\n",
      "epoch: 2, loss: 1.858319884678889, acc: 0.603\n",
      "epoch: 2, loss: 1.8302946389842871, acc: 0.6345\n",
      "epoch: 2, loss: 1.8485253673681954, acc: 0.5875\n",
      "epoch: 2, loss: 1.8370298824485576, acc: 0.602\n",
      "epoch: 2, loss: 1.8452304731145228, acc: 0.578\n",
      "epoch: 2, loss: 1.81051620244918, acc: 0.6185\n",
      "epoch: 2, loss: 1.79456485091689, acc: 0.622\n",
      "epoch: 2, loss: 1.7998128274493934, acc: 0.608\n",
      "epoch: 2, loss: 1.780107775996088, acc: 0.617\n",
      "epoch: 2, loss: 1.7901396781714665, acc: 0.6075\n",
      "epoch: 2, loss: 1.7831903056510423, acc: 0.6075\n",
      "epoch: 2, loss: 1.7796993305621749, acc: 0.604\n",
      "epoch: 2, loss: 1.7833150717007424, acc: 0.6015\n",
      "epoch: 2, loss: 1.76230600293688, acc: 0.6175\n",
      "n_batches 24\n",
      "epoch: 3, loss: 1.7339180257085995, acc: 0.625\n",
      "epoch: 3, loss: 1.7496185780179876, acc: 0.6205\n",
      "epoch: 3, loss: 1.7372565988326056, acc: 0.6295\n",
      "epoch: 3, loss: 1.7239435730222503, acc: 0.642\n",
      "epoch: 3, loss: 1.707200332446288, acc: 0.6515\n",
      "epoch: 3, loss: 1.7121727394229143, acc: 0.657\n",
      "epoch: 3, loss: 1.7181032609076734, acc: 0.6345\n",
      "epoch: 3, loss: 1.6909580499168677, acc: 0.649\n",
      "epoch: 3, loss: 1.6902286328748948, acc: 0.6535\n",
      "epoch: 3, loss: 1.6718072035524987, acc: 0.65\n",
      "epoch: 3, loss: 1.6710685124683144, acc: 0.6515\n",
      "epoch: 3, loss: 1.6366020653900852, acc: 0.6805\n",
      "epoch: 3, loss: 1.6624456330103103, acc: 0.6305\n",
      "epoch: 3, loss: 1.648933819227234, acc: 0.653\n",
      "epoch: 3, loss: 1.6620319572042708, acc: 0.6315\n",
      "epoch: 3, loss: 1.6238541884031708, acc: 0.6595\n",
      "epoch: 3, loss: 1.604973115074189, acc: 0.67\n",
      "epoch: 3, loss: 1.6097066148270807, acc: 0.6605\n",
      "epoch: 3, loss: 1.5908869853250593, acc: 0.6635\n",
      "epoch: 3, loss: 1.6027105522003475, acc: 0.653\n",
      "epoch: 3, loss: 1.5986841854069025, acc: 0.6475\n",
      "epoch: 3, loss: 1.5931281058631594, acc: 0.6495\n",
      "epoch: 3, loss: 1.5997680984887328, acc: 0.651\n",
      "epoch: 3, loss: 1.5765515860012191, acc: 0.6625\n",
      "n_batches 24\n",
      "epoch: 4, loss: 1.5477703181496207, acc: 0.673\n",
      "epoch: 4, loss: 1.564016876715726, acc: 0.671\n",
      "epoch: 4, loss: 1.552453932738089, acc: 0.672\n",
      "epoch: 4, loss: 1.5390637871849864, acc: 0.69\n",
      "epoch: 4, loss: 1.5214741455096574, acc: 0.6845\n",
      "epoch: 4, loss: 1.5283641855501642, acc: 0.693\n",
      "epoch: 4, loss: 1.537736840880715, acc: 0.666\n",
      "epoch: 4, loss: 1.5059516600273677, acc: 0.689\n",
      "epoch: 4, loss: 1.5097183951743673, acc: 0.689\n",
      "epoch: 4, loss: 1.4890849379014135, acc: 0.687\n",
      "epoch: 4, loss: 1.488997622866281, acc: 0.6855\n",
      "epoch: 4, loss: 1.4507202156143533, acc: 0.7235\n",
      "epoch: 4, loss: 1.4823418609591519, acc: 0.6675\n",
      "epoch: 4, loss: 1.4670999874647477, acc: 0.6905\n",
      "epoch: 4, loss: 1.4840101963503776, acc: 0.671\n",
      "epoch: 4, loss: 1.4462804456296738, acc: 0.693\n",
      "epoch: 4, loss: 1.4238210755194955, acc: 0.7115\n",
      "epoch: 4, loss: 1.4289561171262286, acc: 0.695\n",
      "epoch: 4, loss: 1.4111884337307625, acc: 0.7025\n",
      "epoch: 4, loss: 1.4237473375553764, acc: 0.695\n",
      "epoch: 4, loss: 1.4232838733344817, acc: 0.6985\n",
      "epoch: 4, loss: 1.4174714608820218, acc: 0.6865\n",
      "epoch: 4, loss: 1.4250122842606894, acc: 0.6855\n",
      "epoch: 4, loss: 1.4008732612307981, acc: 0.701\n",
      "n_batches 24\n",
      "epoch: 5, loss: 1.374544495871557, acc: 0.7085\n",
      "epoch: 5, loss: 1.3899929972428948, acc: 0.7055\n",
      "epoch: 5, loss: 1.3794703720795563, acc: 0.715\n",
      "epoch: 5, loss: 1.3660889099740006, acc: 0.7245\n",
      "epoch: 5, loss: 1.349255336482498, acc: 0.717\n",
      "epoch: 5, loss: 1.3573502677662823, acc: 0.7295\n",
      "epoch: 5, loss: 1.3707251784810532, acc: 0.691\n",
      "epoch: 5, loss: 1.3346817252915717, acc: 0.725\n",
      "epoch: 5, loss: 1.3430794423817585, acc: 0.7205\n",
      "epoch: 5, loss: 1.3219282011155524, acc: 0.713\n",
      "epoch: 5, loss: 1.322357160876068, acc: 0.7155\n",
      "epoch: 5, loss: 1.2817718960548714, acc: 0.7525\n",
      "epoch: 5, loss: 1.3176776539677855, acc: 0.699\n",
      "epoch: 5, loss: 1.3014335826384837, acc: 0.7225\n",
      "epoch: 5, loss: 1.3211819370575173, acc: 0.7055\n",
      "epoch: 5, loss: 1.2868504991426362, acc: 0.721\n",
      "epoch: 5, loss: 1.2608937070336468, acc: 0.7435\n",
      "epoch: 5, loss: 1.2665053540675764, acc: 0.735\n",
      "epoch: 5, loss: 1.2501177765343932, acc: 0.728\n",
      "epoch: 5, loss: 1.2623611777426773, acc: 0.7295\n",
      "epoch: 5, loss: 1.2662368326054119, acc: 0.732\n",
      "epoch: 5, loss: 1.2605679975547046, acc: 0.7185\n",
      "epoch: 5, loss: 1.268000577658846, acc: 0.7255\n",
      "epoch: 5, loss: 1.2441690417536588, acc: 0.734\n",
      "n_batches 24\n",
      "epoch: 6, loss: 1.222172192149724, acc: 0.7365\n",
      "epoch: 6, loss: 1.235607805000673, acc: 0.7335\n",
      "epoch: 6, loss: 1.226300370957565, acc: 0.7545\n",
      "epoch: 6, loss: 1.2131126235519258, acc: 0.756\n",
      "epoch: 6, loss: 1.1973938916272344, acc: 0.7465\n",
      "epoch: 6, loss: 1.206860842999807, acc: 0.755\n",
      "epoch: 6, loss: 1.2237293059641827, acc: 0.717\n",
      "epoch: 6, loss: 1.184601364882403, acc: 0.752\n",
      "epoch: 6, loss: 1.1979800973866754, acc: 0.7535\n",
      "epoch: 6, loss: 1.1765642559108027, acc: 0.744\n",
      "epoch: 6, loss: 1.1774863050600075, acc: 0.74\n",
      "epoch: 6, loss: 1.1356671235902995, acc: 0.7705\n",
      "epoch: 6, loss: 1.1755201396313568, acc: 0.7375\n",
      "epoch: 6, loss: 1.1580560064354855, acc: 0.749\n",
      "epoch: 6, loss: 1.179745510510553, acc: 0.7355\n",
      "epoch: 6, loss: 1.1508385263189824, acc: 0.746\n",
      "epoch: 6, loss: 1.1213934145831208, acc: 0.7695\n",
      "epoch: 6, loss: 1.1276431817248527, acc: 0.763\n",
      "epoch: 6, loss: 1.1129097358870126, acc: 0.759\n",
      "epoch: 6, loss: 1.1244669448793387, acc: 0.7585\n",
      "epoch: 6, loss: 1.1321577681682897, acc: 0.7625\n",
      "epoch: 6, loss: 1.126860810016763, acc: 0.7455\n",
      "epoch: 6, loss: 1.1334524519872253, acc: 0.7545\n",
      "epoch: 6, loss: 1.1110881264580694, acc: 0.7625\n",
      "n_batches 24\n",
      "epoch: 7, loss: 1.0937919711027526, acc: 0.755\n",
      "epoch: 7, loss: 1.1049104244814116, acc: 0.7575\n",
      "epoch: 7, loss: 1.0959908369560836, acc: 0.7755\n",
      "epoch: 7, loss: 1.08357781051051, acc: 0.7855\n",
      "epoch: 7, loss: 1.0688753446455652, acc: 0.7725\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 7, loss: 1.080018087046367, acc: 0.7725\n",
      "epoch: 7, loss: 1.0993428181170593, acc: 0.74\n",
      "epoch: 7, loss: 1.0585372736845953, acc: 0.772\n",
      "epoch: 7, loss: 1.0765449483328786, acc: 0.7725\n",
      "epoch: 7, loss: 1.0550589503779857, acc: 0.766\n",
      "epoch: 7, loss: 1.0564096761401016, acc: 0.767\n",
      "epoch: 7, loss: 1.0136238756188245, acc: 0.7895\n",
      "epoch: 7, loss: 1.057075672778214, acc: 0.76\n",
      "epoch: 7, loss: 1.038001165122657, acc: 0.7785\n",
      "epoch: 7, loss: 1.060515449598771, acc: 0.7625\n",
      "epoch: 7, loss: 1.03817966818053, acc: 0.7645\n",
      "epoch: 7, loss: 1.0054162014752603, acc: 0.7825\n",
      "epoch: 7, loss: 1.0120713357790658, acc: 0.783\n",
      "epoch: 7, loss: 0.9991727898392814, acc: 0.779\n",
      "epoch: 7, loss: 1.009700775449356, acc: 0.7745\n",
      "epoch: 7, loss: 1.0204072365946395, acc: 0.773\n",
      "epoch: 7, loss: 1.0155160206331864, acc: 0.759\n",
      "epoch: 7, loss: 1.0210220152722778, acc: 0.7755\n",
      "epoch: 7, loss: 1.0002928261257902, acc: 0.787\n",
      "n_batches 24\n",
      "epoch: 8, loss: 0.9877566051170847, acc: 0.7675\n",
      "epoch: 8, loss: 0.9962698110191561, acc: 0.776\n",
      "epoch: 8, loss: 0.9874485706385624, acc: 0.786\n",
      "epoch: 8, loss: 0.9758183085042996, acc: 0.8035\n",
      "epoch: 8, loss: 0.9621167474594686, acc: 0.794\n",
      "epoch: 8, loss: 0.9747766252879337, acc: 0.7875\n",
      "epoch: 8, loss: 0.9959053248111956, acc: 0.758\n",
      "epoch: 8, loss: 0.9542561415522579, acc: 0.789\n",
      "epoch: 8, loss: 0.9762379572201673, acc: 0.783\n",
      "epoch: 8, loss: 0.9548027782060723, acc: 0.786\n",
      "epoch: 8, loss: 0.9564123202796924, acc: 0.7865\n",
      "epoch: 8, loss: 0.9129919572627964, acc: 0.8125\n",
      "epoch: 8, loss: 0.959212273941651, acc: 0.7775\n",
      "epoch: 8, loss: 0.93868083966524, acc: 0.803\n",
      "epoch: 8, loss: 0.9610562080433942, acc: 0.782\n",
      "epoch: 8, loss: 0.9455721528328729, acc: 0.786\n",
      "epoch: 8, loss: 0.9099218856192446, acc: 0.7995\n",
      "epoch: 8, loss: 0.916744022105077, acc: 0.7995\n",
      "epoch: 8, loss: 0.9055680577572582, acc: 0.792\n",
      "epoch: 8, loss: 0.9147910070799619, acc: 0.7895\n",
      "epoch: 8, loss: 0.9284998003106788, acc: 0.7955\n",
      "epoch: 8, loss: 0.9234342143652081, acc: 0.7755\n",
      "epoch: 8, loss: 0.9276877949993177, acc: 0.7975\n",
      "epoch: 8, loss: 0.9087990283738285, acc: 0.8005\n",
      "n_batches 24\n",
      "epoch: 9, loss: 0.9004923835770601, acc: 0.786\n",
      "epoch: 9, loss: 0.9067458467284464, acc: 0.79\n",
      "epoch: 9, loss: 0.8977043036849754, acc: 0.8005\n",
      "epoch: 9, loss: 0.887074514040927, acc: 0.8145\n",
      "epoch: 9, loss: 0.8741939739505834, acc: 0.804\n",
      "epoch: 9, loss: 0.8881992063242344, acc: 0.799\n",
      "epoch: 9, loss: 0.9104935540297865, acc: 0.778\n",
      "epoch: 9, loss: 0.8685361835665633, acc: 0.8035\n",
      "epoch: 9, loss: 0.8941419336895458, acc: 0.7975\n",
      "epoch: 9, loss: 0.8729679408631749, acc: 0.795\n",
      "epoch: 9, loss: 0.8746548338139001, acc: 0.7965\n",
      "epoch: 9, loss: 0.8306089673719808, acc: 0.8255\n",
      "epoch: 9, loss: 0.8787609730881244, acc: 0.788\n",
      "epoch: 9, loss: 0.8571717099736366, acc: 0.812\n",
      "epoch: 9, loss: 0.8792901661044172, acc: 0.7965\n",
      "epoch: 9, loss: 0.8697069742848681, acc: 0.8\n",
      "epoch: 9, loss: 0.8317854153494085, acc: 0.8125\n",
      "epoch: 9, loss: 0.8387780666942004, acc: 0.8135\n",
      "epoch: 9, loss: 0.8291820693659879, acc: 0.805\n",
      "epoch: 9, loss: 0.8370873049496198, acc: 0.8025\n",
      "epoch: 9, loss: 0.853404463936303, acc: 0.804\n",
      "epoch: 9, loss: 0.8477264602416499, acc: 0.788\n",
      "epoch: 9, loss: 0.8509097895875896, acc: 0.809\n",
      "epoch: 9, loss: 0.8338170907752397, acc: 0.8115\n",
      "n_batches 24\n",
      "epoch: 10, loss: 0.8293198486225575, acc: 0.7945\n",
      "epoch: 10, loss: 0.8336666787474751, acc: 0.8015\n",
      "epoch: 10, loss: 0.8241283571659938, acc: 0.817\n",
      "epoch: 10, loss: 0.8145360998772755, acc: 0.822\n",
      "epoch: 10, loss: 0.8023272861394426, acc: 0.812\n",
      "epoch: 10, loss: 0.817473101286077, acc: 0.8105\n",
      "epoch: 10, loss: 0.8402918263634155, acc: 0.7935\n",
      "epoch: 10, loss: 0.798484595713147, acc: 0.815\n",
      "epoch: 10, loss: 0.8273008272496583, acc: 0.807\n",
      "epoch: 10, loss: 0.8062743939821199, acc: 0.8005\n",
      "epoch: 10, loss: 0.8080411319345798, acc: 0.8095\n",
      "epoch: 10, loss: 0.7633522619978732, acc: 0.8375\n",
      "epoch: 10, loss: 0.812722012672795, acc: 0.8\n",
      "epoch: 10, loss: 0.7905577683377789, acc: 0.822\n",
      "epoch: 10, loss: 0.8122148814110683, acc: 0.809\n",
      "epoch: 10, loss: 0.8075469973036224, acc: 0.808\n",
      "epoch: 10, loss: 0.7677924659625933, acc: 0.8195\n",
      "epoch: 10, loss: 0.7750927168738033, acc: 0.824\n",
      "epoch: 10, loss: 0.7666964267228843, acc: 0.8185\n",
      "epoch: 10, loss: 0.7734999834192878, acc: 0.8105\n",
      "epoch: 10, loss: 0.7919573088686112, acc: 0.8125\n",
      "epoch: 10, loss: 0.7854716404188746, acc: 0.7985\n",
      "epoch: 10, loss: 0.7877082204780183, acc: 0.8175\n",
      "epoch: 10, loss: 0.772407620963638, acc: 0.8215\n",
      "n_batches 24\n",
      "epoch: 11, loss: 0.7710370697231922, acc: 0.8035\n",
      "epoch: 11, loss: 0.7739385812222962, acc: 0.8125\n",
      "epoch: 11, loss: 0.7634896651645535, acc: 0.8265\n",
      "epoch: 11, loss: 0.7550703562269909, acc: 0.8315\n",
      "epoch: 11, loss: 0.7433746468036966, acc: 0.824\n",
      "epoch: 11, loss: 0.7595003049480543, acc: 0.82\n",
      "epoch: 11, loss: 0.7825428404304212, acc: 0.805\n",
      "epoch: 11, loss: 0.740866606155796, acc: 0.825\n",
      "epoch: 11, loss: 0.7725477914908248, acc: 0.814\n",
      "epoch: 11, loss: 0.7518059689814532, acc: 0.81\n",
      "epoch: 11, loss: 0.7533183741922305, acc: 0.8155\n",
      "epoch: 11, loss: 0.7081002261221325, acc: 0.84\n",
      "epoch: 11, loss: 0.7581531365634621, acc: 0.8065\n",
      "epoch: 11, loss: 0.7358002338089139, acc: 0.828\n",
      "epoch: 11, loss: 0.7569270283569524, acc: 0.8155\n",
      "epoch: 11, loss: 0.7561632567569746, acc: 0.818\n",
      "epoch: 11, loss: 0.714982688611321, acc: 0.8295\n",
      "epoch: 11, loss: 0.7227803550406935, acc: 0.832\n",
      "epoch: 11, loss: 0.7152488779691057, acc: 0.8285\n",
      "epoch: 11, loss: 0.7210799405478451, acc: 0.8175\n",
      "epoch: 11, loss: 0.7414539801626479, acc: 0.82\n",
      "epoch: 11, loss: 0.7339646419762909, acc: 0.81\n",
      "epoch: 11, loss: 0.7354775268171229, acc: 0.828\n",
      "epoch: 11, loss: 0.7217257861920264, acc: 0.828\n",
      "n_batches 24\n",
      "epoch: 12, loss: 0.7229633139002418, acc: 0.8135\n",
      "epoch: 12, loss: 0.7246058064190326, acc: 0.82\n",
      "epoch: 12, loss: 0.713367166100267, acc: 0.832\n",
      "epoch: 12, loss: 0.7059290720671207, acc: 0.842\n",
      "epoch: 12, loss: 0.6946526280891366, acc: 0.8305\n",
      "epoch: 12, loss: 0.7114131291855635, acc: 0.8295\n",
      "epoch: 12, loss: 0.7346796447362802, acc: 0.8085\n",
      "epoch: 12, loss: 0.6930410682749757, acc: 0.8335\n",
      "epoch: 12, loss: 0.7273635886788262, acc: 0.8215\n",
      "epoch: 12, loss: 0.7068652877970796, acc: 0.8165\n",
      "epoch: 12, loss: 0.7081186952238863, acc: 0.823\n",
      "epoch: 12, loss: 0.6622880223755793, acc: 0.8465\n",
      "epoch: 12, loss: 0.7126573393603289, acc: 0.813\n",
      "epoch: 12, loss: 0.6903334622088637, acc: 0.833\n",
      "epoch: 12, loss: 0.710977891733672, acc: 0.825\n",
      "epoch: 12, loss: 0.7133111559211965, acc: 0.826\n",
      "epoch: 12, loss: 0.6709231280331069, acc: 0.835\n",
      "epoch: 12, loss: 0.679471564564798, acc: 0.84\n",
      "epoch: 12, loss: 0.6724294137853392, acc: 0.836\n",
      "epoch: 12, loss: 0.6773886146731584, acc: 0.8255\n",
      "epoch: 12, loss: 0.6994394693337568, acc: 0.828\n",
      "epoch: 12, loss: 0.6908374984540944, acc: 0.8185\n",
      "epoch: 12, loss: 0.6918723279642325, acc: 0.8355\n",
      "epoch: 12, loss: 0.6793648956685797, acc: 0.8325\n",
      "n_batches 24\n",
      "epoch: 13, loss: 0.6828137606156194, acc: 0.822\n",
      "epoch: 13, loss: 0.6833695176458873, acc: 0.8285\n",
      "epoch: 13, loss: 0.6715408485659108, acc: 0.8375\n",
      "epoch: 13, loss: 0.6648491720544016, acc: 0.847\n",
      "epoch: 13, loss: 0.6539411640572139, acc: 0.84\n",
      "epoch: 13, loss: 0.6710884959888176, acc: 0.837\n",
      "epoch: 13, loss: 0.6945189963191211, acc: 0.814\n",
      "epoch: 13, loss: 0.6528595577055645, acc: 0.8425\n",
      "epoch: 13, loss: 0.6896003009965924, acc: 0.824\n",
      "epoch: 13, loss: 0.6693044450720297, acc: 0.821\n",
      "epoch: 13, loss: 0.6704601542789862, acc: 0.8295\n",
      "epoch: 13, loss: 0.6238210789478023, acc: 0.8505\n",
      "epoch: 13, loss: 0.6742302370565583, acc: 0.824\n",
      "epoch: 13, loss: 0.6522160677562987, acc: 0.841\n",
      "epoch: 13, loss: 0.6723281393252132, acc: 0.831\n",
      "epoch: 13, loss: 0.6770886956002499, acc: 0.832\n",
      "epoch: 13, loss: 0.6337957336022088, acc: 0.841\n",
      "epoch: 13, loss: 0.6431191318635238, acc: 0.8455\n",
      "epoch: 13, loss: 0.6362553519981188, acc: 0.8395\n",
      "epoch: 13, loss: 0.6405359929739481, acc: 0.8335\n",
      "epoch: 13, loss: 0.6640225763096373, acc: 0.8365\n",
      "epoch: 13, loss: 0.6543770367042998, acc: 0.8265\n",
      "epoch: 13, loss: 0.6550327529923232, acc: 0.838\n",
      "epoch: 13, loss: 0.6435319892354594, acc: 0.8395\n",
      "n_batches 24\n",
      "epoch: 14, loss: 0.6487634140021145, acc: 0.8285\n",
      "epoch: 14, loss: 0.6485140304487566, acc: 0.8325\n",
      "epoch: 14, loss: 0.6361637360494403, acc: 0.842\n",
      "epoch: 14, loss: 0.6300774061721935, acc: 0.8495\n",
      "epoch: 14, loss: 0.6195193782814338, acc: 0.846\n",
      "epoch: 14, loss: 0.636925676524621, acc: 0.8395\n",
      "epoch: 14, loss: 0.6604116084316591, acc: 0.819\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 14, loss: 0.6186983019084359, acc: 0.849\n",
      "epoch: 14, loss: 0.6577039247593232, acc: 0.829\n",
      "epoch: 14, loss: 0.637472324929161, acc: 0.8265\n",
      "epoch: 14, loss: 0.6387239337765952, acc: 0.8385\n",
      "epoch: 14, loss: 0.5911534009619731, acc: 0.856\n",
      "epoch: 14, loss: 0.6413112127950558, acc: 0.8295\n",
      "epoch: 14, loss: 0.6198319383867239, acc: 0.8445\n",
      "epoch: 14, loss: 0.6394218406443274, acc: 0.8385\n",
      "epoch: 14, loss: 0.6461095776465069, acc: 0.8365\n",
      "epoch: 14, loss: 0.6021470935299693, acc: 0.846\n",
      "epoch: 14, loss: 0.6121443033940606, acc: 0.851\n",
      "epoch: 14, loss: 0.6053262076249826, acc: 0.844\n",
      "epoch: 14, loss: 0.6090792213344082, acc: 0.84\n",
      "epoch: 14, loss: 0.6338517063885369, acc: 0.843\n",
      "epoch: 14, loss: 0.623210534514651, acc: 0.834\n",
      "epoch: 14, loss: 0.6235620712625194, acc: 0.843\n",
      "epoch: 14, loss: 0.612799610910495, acc: 0.8445\n",
      "n_batches 24\n",
      "epoch: 15, loss: 0.6195248015512456, acc: 0.8345\n",
      "epoch: 15, loss: 0.618705310911914, acc: 0.841\n",
      "epoch: 15, loss: 0.6058797932516933, acc: 0.8465\n",
      "epoch: 15, loss: 0.6003549263055031, acc: 0.8535\n",
      "epoch: 15, loss: 0.5900610430347181, acc: 0.8535\n",
      "epoch: 15, loss: 0.6076792797262732, acc: 0.8435\n",
      "epoch: 15, loss: 0.6310977743740059, acc: 0.8255\n",
      "epoch: 15, loss: 0.5893206943643149, acc: 0.852\n",
      "epoch: 15, loss: 0.6303825044070801, acc: 0.8315\n",
      "epoch: 15, loss: 0.6101857221487319, acc: 0.8315\n",
      "epoch: 15, loss: 0.6115954933060843, acc: 0.841\n",
      "epoch: 15, loss: 0.5631041971067449, acc: 0.8615\n",
      "epoch: 15, loss: 0.6127560835559639, acc: 0.835\n",
      "epoch: 15, loss: 0.5919756602593633, acc: 0.85\n",
      "epoch: 15, loss: 0.6111316256467567, acc: 0.844\n",
      "epoch: 15, loss: 0.6192209814940429, acc: 0.8405\n",
      "epoch: 15, loss: 0.5748447717513234, acc: 0.851\n",
      "epoch: 15, loss: 0.5854106631592624, acc: 0.856\n",
      "epoch: 15, loss: 0.5785750080153739, acc: 0.849\n",
      "epoch: 15, loss: 0.5819304067026868, acc: 0.8475\n",
      "epoch: 15, loss: 0.607893562260349, acc: 0.847\n",
      "epoch: 15, loss: 0.5962635834255028, acc: 0.84\n",
      "epoch: 15, loss: 0.5963723940473021, acc: 0.8495\n",
      "epoch: 15, loss: 0.5861774492403222, acc: 0.8495\n",
      "n_batches 24\n",
      "epoch: 16, loss: 0.5941738686052979, acc: 0.84\n",
      "epoch: 16, loss: 0.5929338934019386, acc: 0.8465\n",
      "epoch: 16, loss: 0.5796778120981775, acc: 0.8525\n",
      "epoch: 16, loss: 0.5746761459709205, acc: 0.8585\n",
      "epoch: 16, loss: 0.5645456943995336, acc: 0.8575\n",
      "epoch: 16, loss: 0.5823608635523365, acc: 0.848\n",
      "epoch: 16, loss: 0.6055978457590836, acc: 0.8295\n",
      "epoch: 16, loss: 0.5637661080864557, acc: 0.8555\n",
      "epoch: 16, loss: 0.6067106326231998, acc: 0.833\n",
      "epoch: 16, loss: 0.5865763356111033, acc: 0.8385\n",
      "epoch: 16, loss: 0.5881730493628923, acc: 0.845\n",
      "epoch: 16, loss: 0.5387702935623067, acc: 0.866\n",
      "epoch: 16, loss: 0.5877484065877596, acc: 0.843\n",
      "epoch: 16, loss: 0.5677778391978348, acc: 0.8555\n",
      "epoch: 16, loss: 0.5865556365582449, acc: 0.8475\n",
      "epoch: 16, loss: 0.5956672647524303, acc: 0.842\n",
      "epoch: 16, loss: 0.551047880602138, acc: 0.8555\n",
      "epoch: 16, loss: 0.5620847795436853, acc: 0.8585\n",
      "epoch: 16, loss: 0.5551597079539716, acc: 0.8535\n",
      "epoch: 16, loss: 0.5582672957342254, acc: 0.8525\n",
      "epoch: 16, loss: 0.585305801046582, acc: 0.852\n",
      "epoch: 16, loss: 0.5727116207157763, acc: 0.8445\n",
      "epoch: 16, loss: 0.5726554486443056, acc: 0.85\n",
      "epoch: 16, loss: 0.5628891217826933, acc: 0.855\n",
      "n_batches 24\n",
      "epoch: 17, loss: 0.5719396187296271, acc: 0.8435\n",
      "epoch: 17, loss: 0.5704406605465907, acc: 0.85\n",
      "epoch: 17, loss: 0.5567886471824153, acc: 0.8575\n",
      "epoch: 17, loss: 0.55228718399898, acc: 0.862\n",
      "epoch: 17, loss: 0.5422390902449868, acc: 0.862\n",
      "epoch: 17, loss: 0.5602315100369389, acc: 0.851\n",
      "epoch: 17, loss: 0.5832404907880956, acc: 0.8325\n",
      "epoch: 17, loss: 0.5413587551423296, acc: 0.859\n",
      "epoch: 17, loss: 0.585969798884134, acc: 0.837\n",
      "epoch: 17, loss: 0.5659512302617239, acc: 0.8435\n",
      "epoch: 17, loss: 0.5677472503620936, acc: 0.849\n",
      "epoch: 17, loss: 0.5174895523346152, acc: 0.8685\n",
      "epoch: 17, loss: 0.5656566267001822, acc: 0.848\n",
      "epoch: 17, loss: 0.5465542477581898, acc: 0.86\n",
      "epoch: 17, loss: 0.5650315172310402, acc: 0.85\n",
      "epoch: 17, loss: 0.5748681841276848, acc: 0.8475\n",
      "epoch: 17, loss: 0.5301246160781011, acc: 0.8595\n",
      "epoch: 17, loss: 0.5415739022977955, acc: 0.863\n",
      "epoch: 17, loss: 0.5345202629697153, acc: 0.861\n",
      "epoch: 17, loss: 0.5374795743981763, acc: 0.857\n",
      "epoch: 17, loss: 0.5654918259181145, acc: 0.857\n",
      "epoch: 17, loss: 0.5519700713710859, acc: 0.849\n",
      "epoch: 17, loss: 0.55185523399922, acc: 0.8535\n",
      "epoch: 17, loss: 0.5423419357493887, acc: 0.858\n",
      "n_batches 24\n",
      "epoch: 18, loss: 0.5522733533347703, acc: 0.848\n",
      "epoch: 18, loss: 0.5506729723435344, acc: 0.8545\n",
      "epoch: 18, loss: 0.5366106787762769, acc: 0.861\n",
      "epoch: 18, loss: 0.5326180144725323, acc: 0.8635\n",
      "epoch: 18, loss: 0.5226119875608043, acc: 0.8655\n",
      "epoch: 18, loss: 0.5407453990761125, acc: 0.8555\n",
      "epoch: 18, loss: 0.56343740101048, acc: 0.8355\n",
      "epoch: 18, loss: 0.5215783987252095, acc: 0.8615\n",
      "epoch: 18, loss: 0.5676443217669513, acc: 0.8385\n",
      "epoch: 18, loss: 0.5477503948864837, acc: 0.846\n",
      "epoch: 18, loss: 0.5497590113087695, acc: 0.8515\n",
      "epoch: 18, loss: 0.49871498401523867, acc: 0.873\n",
      "epoch: 18, loss: 0.5459716838789567, acc: 0.855\n",
      "epoch: 18, loss: 0.5277919930369721, acc: 0.8625\n",
      "epoch: 18, loss: 0.546047243519248, acc: 0.8525\n",
      "epoch: 18, loss: 0.5563672306790514, acc: 0.8515\n",
      "epoch: 18, loss: 0.5115890813936299, acc: 0.863\n",
      "epoch: 18, loss: 0.5233892392989057, acc: 0.864\n",
      "epoch: 18, loss: 0.5162173165626819, acc: 0.8635\n",
      "epoch: 18, loss: 0.519072579056692, acc: 0.86\n",
      "epoch: 18, loss: 0.5479732022203815, acc: 0.8605\n",
      "epoch: 18, loss: 0.5335770869300764, acc: 0.8545\n",
      "epoch: 18, loss: 0.5334587623052793, acc: 0.859\n",
      "epoch: 18, loss: 0.5240626288070203, acc: 0.8605\n",
      "n_batches 24\n",
      "epoch: 19, loss: 0.5347536905587342, acc: 0.85\n",
      "epoch: 19, loss: 0.533131972256054, acc: 0.8565\n",
      "epoch: 19, loss: 0.518689079252274, acc: 0.865\n",
      "epoch: 19, loss: 0.5151661900953486, acc: 0.868\n",
      "epoch: 19, loss: 0.5052063543497138, acc: 0.8685\n",
      "epoch: 19, loss: 0.5234364518313269, acc: 0.858\n",
      "epoch: 19, loss: 0.5457797182385439, acc: 0.8415\n",
      "epoch: 19, loss: 0.5040072330015123, acc: 0.8655\n",
      "epoch: 19, loss: 0.5513451830500871, acc: 0.844\n",
      "epoch: 19, loss: 0.5315854230781597, acc: 0.8505\n",
      "epoch: 19, loss: 0.5338019402169714, acc: 0.8575\n",
      "epoch: 19, loss: 0.4820172599114702, acc: 0.8735\n",
      "epoch: 19, loss: 0.5283400501359694, acc: 0.8605\n",
      "epoch: 19, loss: 0.5111074173642641, acc: 0.866\n",
      "epoch: 19, loss: 0.5291640111120911, acc: 0.855\n",
      "epoch: 19, loss: 0.5398107943151109, acc: 0.8555\n",
      "epoch: 19, loss: 0.4950653636396355, acc: 0.8695\n",
      "epoch: 19, loss: 0.5071238286612594, acc: 0.8675\n",
      "epoch: 19, loss: 0.49984947480901615, acc: 0.865\n",
      "epoch: 19, loss: 0.5026396763370464, acc: 0.863\n",
      "epoch: 19, loss: 0.5323698898733705, acc: 0.863\n",
      "epoch: 19, loss: 0.5171597762835142, acc: 0.8585\n",
      "epoch: 19, loss: 0.5170908189523513, acc: 0.862\n",
      "epoch: 19, loss: 0.5077061438089173, acc: 0.863\n",
      "n_batches 24\n",
      "epoch: 20, loss: 0.5190025772780003, acc: 0.8545\n",
      "epoch: 20, loss: 0.5174641383943024, acc: 0.86\n",
      "epoch: 20, loss: 0.5026817047456177, acc: 0.868\n",
      "epoch: 20, loss: 0.4995944474655489, acc: 0.873\n",
      "epoch: 20, loss: 0.4896488473931682, acc: 0.8735\n",
      "epoch: 20, loss: 0.507981808789707, acc: 0.8605\n",
      "epoch: 20, loss: 0.5299455849317326, acc: 0.8485\n",
      "epoch: 20, loss: 0.48825519831331704, acc: 0.869\n",
      "epoch: 20, loss: 0.5367284537829511, acc: 0.8475\n",
      "epoch: 20, loss: 0.5171536605107094, acc: 0.853\n",
      "epoch: 20, loss: 0.5195679841562384, acc: 0.8595\n",
      "epoch: 20, loss: 0.46707491959906117, acc: 0.879\n",
      "epoch: 20, loss: 0.512444141388, acc: 0.863\n",
      "epoch: 20, loss: 0.4961330284021539, acc: 0.8685\n",
      "epoch: 20, loss: 0.5140672808081902, acc: 0.8585\n",
      "epoch: 20, loss: 0.5248829924013138, acc: 0.8565\n",
      "epoch: 20, loss: 0.4802578306099589, acc: 0.874\n",
      "epoch: 20, loss: 0.4924768957162397, acc: 0.868\n",
      "epoch: 20, loss: 0.4851167323026281, acc: 0.869\n",
      "epoch: 20, loss: 0.48787528371202404, acc: 0.8665\n",
      "epoch: 20, loss: 0.5183655438457851, acc: 0.8665\n",
      "epoch: 20, loss: 0.502436938665889, acc: 0.86\n",
      "epoch: 20, loss: 0.5024678968201475, acc: 0.8655\n",
      "epoch: 20, loss: 0.4930038309319026, acc: 0.868\n",
      "n_batches 24\n",
      "epoch: 21, loss: 0.5047832209468971, acc: 0.861\n",
      "epoch: 21, loss: 0.5033968924588264, acc: 0.864\n",
      "epoch: 21, loss: 0.48827900030672244, acc: 0.8735\n",
      "epoch: 21, loss: 0.48561667721762125, acc: 0.876\n",
      "epoch: 21, loss: 0.4756558434581781, acc: 0.8745\n",
      "epoch: 21, loss: 0.49409903834558755, acc: 0.8635\n",
      "epoch: 21, loss: 0.5156634450629449, acc: 0.8525\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 21, loss: 0.4740829440399786, acc: 0.872\n",
      "epoch: 21, loss: 0.5235416380783438, acc: 0.851\n",
      "epoch: 21, loss: 0.5041845528415357, acc: 0.857\n",
      "epoch: 21, loss: 0.506772972770014, acc: 0.8635\n",
      "epoch: 21, loss: 0.4536423432121635, acc: 0.881\n",
      "epoch: 21, loss: 0.4980322484823531, acc: 0.8645\n",
      "epoch: 21, loss: 0.482636468278811, acc: 0.87\n",
      "epoch: 21, loss: 0.5004948980322793, acc: 0.859\n",
      "epoch: 21, loss: 0.5113610754262815, acc: 0.8585\n",
      "epoch: 21, loss: 0.466890024802701, acc: 0.8745\n",
      "epoch: 21, loss: 0.47920984310684467, acc: 0.871\n",
      "epoch: 21, loss: 0.4717666266299756, acc: 0.8745\n",
      "epoch: 21, loss: 0.4745345760642045, acc: 0.8705\n",
      "epoch: 21, loss: 0.5057216316404679, acc: 0.8695\n",
      "epoch: 21, loss: 0.4891737234646627, acc: 0.862\n",
      "epoch: 21, loss: 0.489312928204205, acc: 0.869\n",
      "epoch: 21, loss: 0.47970361465557687, acc: 0.8725\n",
      "n_batches 24\n",
      "epoch: 22, loss: 0.4918812882499841, acc: 0.864\n",
      "epoch: 22, loss: 0.4906939676542045, acc: 0.8665\n",
      "epoch: 22, loss: 0.47523659336679175, acc: 0.8755\n",
      "epoch: 22, loss: 0.4730089274115695, acc: 0.8815\n",
      "epoch: 22, loss: 0.4629902901626972, acc: 0.8765\n",
      "epoch: 22, loss: 0.4815609186129912, acc: 0.8645\n",
      "epoch: 22, loss: 0.5026877378529542, acc: 0.855\n",
      "epoch: 22, loss: 0.4612619245420383, acc: 0.875\n",
      "epoch: 22, loss: 0.5115639240815456, acc: 0.8555\n",
      "epoch: 22, loss: 0.49245504162934317, acc: 0.861\n",
      "epoch: 22, loss: 0.49519993693473535, acc: 0.868\n",
      "epoch: 22, loss: 0.441506271132004, acc: 0.8845\n",
      "epoch: 22, loss: 0.4849070049095299, acc: 0.868\n",
      "epoch: 22, loss: 0.47043197073266174, acc: 0.872\n",
      "epoch: 22, loss: 0.4882174365699921, acc: 0.859\n",
      "epoch: 22, loss: 0.49904097723037166, acc: 0.863\n",
      "epoch: 22, loss: 0.4547943164571781, acc: 0.877\n",
      "epoch: 22, loss: 0.46715417262598546, acc: 0.874\n",
      "epoch: 22, loss: 0.45963856428122385, acc: 0.877\n",
      "epoch: 22, loss: 0.46243188525104734, acc: 0.8705\n",
      "epoch: 22, loss: 0.4942528132367311, acc: 0.8705\n",
      "epoch: 22, loss: 0.4771560380480463, acc: 0.8665\n",
      "epoch: 22, loss: 0.47742469989672703, acc: 0.874\n",
      "epoch: 22, loss: 0.4676332670938195, acc: 0.8725\n",
      "n_batches 24\n",
      "epoch: 23, loss: 0.48011280493310143, acc: 0.8675\n",
      "epoch: 23, loss: 0.479177626876841, acc: 0.867\n",
      "epoch: 23, loss: 0.46339051329320147, acc: 0.8775\n",
      "epoch: 23, loss: 0.46159752280163113, acc: 0.884\n",
      "epoch: 23, loss: 0.4514986883933246, acc: 0.8775\n",
      "epoch: 23, loss: 0.4701754893955454, acc: 0.866\n",
      "epoch: 23, loss: 0.49086907508468064, acc: 0.858\n",
      "epoch: 23, loss: 0.4495942226084406, acc: 0.877\n",
      "epoch: 23, loss: 0.5006516825786711, acc: 0.8575\n",
      "epoch: 23, loss: 0.48180822355742975, acc: 0.865\n",
      "epoch: 23, loss: 0.484686290303993, acc: 0.869\n",
      "epoch: 23, loss: 0.43047672785056024, acc: 0.887\n",
      "epoch: 23, loss: 0.4728985127414402, acc: 0.8705\n",
      "epoch: 23, loss: 0.45932908577445614, acc: 0.874\n",
      "epoch: 23, loss: 0.4770589688427216, acc: 0.863\n",
      "epoch: 23, loss: 0.4878125914830165, acc: 0.864\n",
      "epoch: 23, loss: 0.443793980322646, acc: 0.8775\n",
      "epoch: 23, loss: 0.4561377291849111, acc: 0.878\n",
      "epoch: 23, loss: 0.44857626413293245, acc: 0.88\n",
      "epoch: 23, loss: 0.4514085179501108, acc: 0.873\n",
      "epoch: 23, loss: 0.48380485932007594, acc: 0.873\n",
      "epoch: 23, loss: 0.4662368348171561, acc: 0.871\n",
      "epoch: 23, loss: 0.46663758079195394, acc: 0.8755\n",
      "epoch: 23, loss: 0.45662925838001034, acc: 0.8745\n",
      "n_batches 24\n",
      "epoch: 24, loss: 0.4693436011382006, acc: 0.87\n",
      "epoch: 24, loss: 0.468709423125879, acc: 0.87\n",
      "epoch: 24, loss: 0.45257278636545206, acc: 0.882\n",
      "epoch: 24, loss: 0.4512159206571997, acc: 0.889\n",
      "epoch: 24, loss: 0.44102590065871294, acc: 0.8805\n",
      "epoch: 24, loss: 0.45979775273340706, acc: 0.8705\n",
      "epoch: 24, loss: 0.480061678524047, acc: 0.8615\n",
      "epoch: 24, loss: 0.4389510238511956, acc: 0.8795\n",
      "epoch: 24, loss: 0.4906434321116186, acc: 0.8605\n",
      "epoch: 24, loss: 0.47210520246905513, acc: 0.87\n",
      "epoch: 24, loss: 0.47508066650098274, acc: 0.871\n",
      "epoch: 24, loss: 0.42040039143484675, acc: 0.8885\n",
      "epoch: 24, loss: 0.4618868782599509, acc: 0.8735\n",
      "epoch: 24, loss: 0.4491845817907526, acc: 0.876\n",
      "epoch: 24, loss: 0.4668869158381794, acc: 0.8665\n",
      "epoch: 24, loss: 0.47750097817944276, acc: 0.867\n",
      "epoch: 24, loss: 0.4337364423527098, acc: 0.8775\n",
      "epoch: 24, loss: 0.4460244028820889, acc: 0.8815\n",
      "epoch: 24, loss: 0.4384279748877283, acc: 0.8825\n",
      "epoch: 24, loss: 0.4413454709104328, acc: 0.8755\n",
      "epoch: 24, loss: 0.4742494043567183, acc: 0.874\n",
      "epoch: 24, loss: 0.45628006348425787, acc: 0.873\n",
      "epoch: 24, loss: 0.4567955577105354, acc: 0.876\n",
      "epoch: 24, loss: 0.4465628478569162, acc: 0.875\n",
      "n_batches 24\n",
      "epoch: 25, loss: 0.45945577188160996, acc: 0.871\n",
      "epoch: 25, loss: 0.4591360755069622, acc: 0.874\n",
      "epoch: 25, loss: 0.44267080130987985, acc: 0.886\n",
      "epoch: 25, loss: 0.44173204239994524, acc: 0.8925\n",
      "epoch: 25, loss: 0.4314351306372639, acc: 0.882\n",
      "epoch: 25, loss: 0.45030325452258535, acc: 0.8735\n",
      "epoch: 25, loss: 0.47014734858685053, acc: 0.8645\n",
      "epoch: 25, loss: 0.4292049878559906, acc: 0.8805\n",
      "epoch: 25, loss: 0.4814437371110745, acc: 0.8625\n",
      "epoch: 25, loss: 0.4632480264863234, acc: 0.871\n",
      "epoch: 25, loss: 0.46627918030822724, acc: 0.8725\n",
      "epoch: 25, loss: 0.41117991109401486, acc: 0.891\n",
      "epoch: 25, loss: 0.4517585257620889, acc: 0.876\n",
      "epoch: 25, loss: 0.4399027781972212, acc: 0.8795\n",
      "epoch: 25, loss: 0.4575642149912753, acc: 0.868\n",
      "epoch: 25, loss: 0.4680367652634185, acc: 0.8695\n",
      "epoch: 25, loss: 0.4245249372817683, acc: 0.8805\n",
      "epoch: 25, loss: 0.43672639911988154, acc: 0.8825\n",
      "epoch: 25, loss: 0.4291085374778107, acc: 0.883\n",
      "epoch: 25, loss: 0.43211371587862907, acc: 0.8755\n",
      "epoch: 25, loss: 0.46547907599157406, acc: 0.8765\n",
      "epoch: 25, loss: 0.44716524695221366, acc: 0.874\n",
      "epoch: 25, loss: 0.44780334088128065, acc: 0.8775\n",
      "epoch: 25, loss: 0.4373346321308528, acc: 0.8765\n",
      "n_batches 24\n",
      "epoch: 26, loss: 0.4503554074236697, acc: 0.874\n",
      "epoch: 26, loss: 0.45036108747862463, acc: 0.876\n",
      "epoch: 26, loss: 0.43356820649064554, acc: 0.8875\n",
      "epoch: 26, loss: 0.4330310740246823, acc: 0.8935\n",
      "epoch: 26, loss: 0.42262648510146017, acc: 0.8845\n",
      "epoch: 26, loss: 0.4415833789161308, acc: 0.876\n",
      "epoch: 26, loss: 0.46103378575479836, acc: 0.8675\n",
      "epoch: 26, loss: 0.42024468615738625, acc: 0.8835\n",
      "epoch: 26, loss: 0.47297171374191227, acc: 0.8645\n",
      "epoch: 26, loss: 0.45512558426202987, acc: 0.872\n",
      "epoch: 26, loss: 0.4581813498247466, acc: 0.8755\n",
      "epoch: 26, loss: 0.40270670024034316, acc: 0.892\n",
      "epoch: 26, loss: 0.44240649312364055, acc: 0.879\n",
      "epoch: 26, loss: 0.4313865566470278, acc: 0.883\n",
      "epoch: 26, loss: 0.44897795588756795, acc: 0.8695\n",
      "epoch: 26, loss: 0.45930316870014976, acc: 0.8725\n",
      "epoch: 26, loss: 0.416053856988289, acc: 0.8815\n",
      "epoch: 26, loss: 0.4281573376021223, acc: 0.8855\n",
      "epoch: 26, loss: 0.42051563460910757, acc: 0.8845\n",
      "epoch: 26, loss: 0.4236115136791483, acc: 0.8785\n",
      "epoch: 26, loss: 0.4574062146665757, acc: 0.8785\n",
      "epoch: 26, loss: 0.43880125469926806, acc: 0.878\n",
      "epoch: 26, loss: 0.43954894669174294, acc: 0.8785\n",
      "epoch: 26, loss: 0.42884731220111644, acc: 0.877\n",
      "n_batches 24\n",
      "epoch: 27, loss: 0.4419351230875802, acc: 0.8755\n",
      "epoch: 27, loss: 0.44227897151666723, acc: 0.879\n",
      "epoch: 27, loss: 0.42517511252941265, acc: 0.89\n",
      "epoch: 27, loss: 0.4250246976740178, acc: 0.8945\n",
      "epoch: 27, loss: 0.41451253277490435, acc: 0.887\n",
      "epoch: 27, loss: 0.43353760681926884, acc: 0.878\n",
      "epoch: 27, loss: 0.45262282481686555, acc: 0.8695\n",
      "epoch: 27, loss: 0.411987887974155, acc: 0.8845\n",
      "epoch: 27, loss: 0.4651251189417161, acc: 0.867\n",
      "epoch: 27, loss: 0.447650735965661, acc: 0.8745\n",
      "epoch: 27, loss: 0.45068896745298564, acc: 0.878\n",
      "epoch: 27, loss: 0.3948860748250735, acc: 0.893\n",
      "epoch: 27, loss: 0.4337500632270467, acc: 0.8815\n",
      "epoch: 27, loss: 0.42353094638888333, acc: 0.8855\n",
      "epoch: 27, loss: 0.4410545123895192, acc: 0.8715\n",
      "epoch: 27, loss: 0.4512084220609488, acc: 0.8725\n",
      "epoch: 27, loss: 0.4082534574379374, acc: 0.8865\n",
      "epoch: 27, loss: 0.4202339476731088, acc: 0.8875\n",
      "epoch: 27, loss: 0.41255822713495427, acc: 0.887\n",
      "epoch: 27, loss: 0.41576090700535756, acc: 0.8795\n",
      "epoch: 27, loss: 0.4499566266596589, acc: 0.8805\n",
      "epoch: 27, loss: 0.43111716343204126, acc: 0.88\n",
      "epoch: 27, loss: 0.43193804183194645, acc: 0.881\n",
      "epoch: 27, loss: 0.42101523821568526, acc: 0.8795\n",
      "n_batches 24\n",
      "epoch: 28, loss: 0.43413080749753913, acc: 0.8765\n",
      "epoch: 28, loss: 0.4348124041011698, acc: 0.8795\n",
      "epoch: 28, loss: 0.4174061804938239, acc: 0.893\n",
      "epoch: 28, loss: 0.4176380833284409, acc: 0.895\n",
      "epoch: 28, loss: 0.4070156349586454, acc: 0.888\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 28, loss: 0.4261046031270131, acc: 0.879\n",
      "epoch: 28, loss: 0.44482576186371714, acc: 0.872\n",
      "epoch: 28, loss: 0.40435474107540875, acc: 0.887\n",
      "epoch: 28, loss: 0.45784899828054515, acc: 0.871\n",
      "epoch: 28, loss: 0.440758352568652, acc: 0.877\n",
      "epoch: 28, loss: 0.4437468617396269, acc: 0.8805\n",
      "epoch: 28, loss: 0.38765666612966343, acc: 0.8945\n",
      "epoch: 28, loss: 0.42572913558537673, acc: 0.8855\n",
      "epoch: 28, loss: 0.4162719237305083, acc: 0.887\n",
      "epoch: 28, loss: 0.43371760887161825, acc: 0.874\n",
      "epoch: 28, loss: 0.44369312509469283, acc: 0.874\n",
      "epoch: 28, loss: 0.4010518183245596, acc: 0.8875\n",
      "epoch: 28, loss: 0.4128799040823506, acc: 0.8875\n",
      "epoch: 28, loss: 0.40517137291182875, acc: 0.889\n",
      "epoch: 28, loss: 0.4085022072233132, acc: 0.882\n",
      "epoch: 28, loss: 0.4430540808357978, acc: 0.881\n",
      "epoch: 28, loss: 0.4240296696321127, acc: 0.881\n",
      "epoch: 28, loss: 0.4249127381568111, acc: 0.8825\n",
      "epoch: 28, loss: 0.4137740433308694, acc: 0.8835\n",
      "n_batches 24\n",
      "epoch: 29, loss: 0.4268817758376639, acc: 0.879\n",
      "epoch: 29, loss: 0.4279043979742051, acc: 0.8795\n",
      "epoch: 29, loss: 0.41020109469334115, acc: 0.895\n",
      "epoch: 29, loss: 0.41080251390420597, acc: 0.8955\n",
      "epoch: 29, loss: 0.4000681931834013, acc: 0.888\n",
      "epoch: 29, loss: 0.4192232252613858, acc: 0.881\n",
      "epoch: 29, loss: 0.4375856421611616, acc: 0.875\n",
      "epoch: 29, loss: 0.39728226668311745, acc: 0.8895\n",
      "epoch: 29, loss: 0.45109346876274264, acc: 0.872\n",
      "epoch: 29, loss: 0.4343825342751162, acc: 0.8765\n",
      "epoch: 29, loss: 0.4372940570796286, acc: 0.8825\n",
      "epoch: 29, loss: 0.380950904375132, acc: 0.8965\n",
      "epoch: 29, loss: 0.4182711643468108, acc: 0.887\n",
      "epoch: 29, loss: 0.4095558616425607, acc: 0.8885\n",
      "epoch: 29, loss: 0.4268954659832483, acc: 0.875\n",
      "epoch: 29, loss: 0.43671331205220304, acc: 0.876\n",
      "epoch: 29, loss: 0.3943850359350238, acc: 0.8905\n",
      "epoch: 29, loss: 0.4060503539092891, acc: 0.8905\n",
      "epoch: 29, loss: 0.3982934546986578, acc: 0.89\n",
      "epoch: 29, loss: 0.4017629113088975, acc: 0.883\n",
      "epoch: 29, loss: 0.436633085994443, acc: 0.8825\n",
      "epoch: 29, loss: 0.4174649135168864, acc: 0.882\n",
      "epoch: 29, loss: 0.418411837640845, acc: 0.8825\n",
      "epoch: 29, loss: 0.40707278519911205, acc: 0.8865\n",
      "n_batches 24\n",
      "epoch: 30, loss: 0.4201345494636854, acc: 0.882\n",
      "epoch: 30, loss: 0.4214913046861103, acc: 0.881\n",
      "epoch: 30, loss: 0.4034884242244971, acc: 0.897\n",
      "epoch: 30, loss: 0.40445814001491803, acc: 0.8975\n",
      "epoch: 30, loss: 0.3936207682910955, acc: 0.8885\n",
      "epoch: 30, loss: 0.4128134502347314, acc: 0.883\n",
      "epoch: 30, loss: 0.4308435652537828, acc: 0.879\n",
      "epoch: 30, loss: 0.3907088813085086, acc: 0.8905\n",
      "epoch: 30, loss: 0.44480041392311953, acc: 0.875\n",
      "epoch: 30, loss: 0.42846127468486006, acc: 0.8785\n",
      "epoch: 30, loss: 0.43127465235773027, acc: 0.8835\n",
      "epoch: 30, loss: 0.37470935888076196, acc: 0.8975\n",
      "epoch: 30, loss: 0.4113213996177972, acc: 0.8895\n",
      "epoch: 30, loss: 0.40332442167964044, acc: 0.8895\n",
      "epoch: 30, loss: 0.4205449108053672, acc: 0.8785\n",
      "epoch: 30, loss: 0.43020747913989826, acc: 0.878\n",
      "epoch: 30, loss: 0.3882016084822591, acc: 0.891\n",
      "epoch: 30, loss: 0.39970019024851844, acc: 0.8915\n",
      "epoch: 30, loss: 0.3918767193096183, acc: 0.891\n",
      "epoch: 30, loss: 0.3954934977086529, acc: 0.8845\n",
      "epoch: 30, loss: 0.43064943178145393, acc: 0.883\n",
      "epoch: 30, loss: 0.41138435639552307, acc: 0.883\n",
      "epoch: 30, loss: 0.41236434033810404, acc: 0.883\n",
      "epoch: 30, loss: 0.4008505922201468, acc: 0.8875\n",
      "n_batches 24\n",
      "epoch: 31, loss: 0.4138406971487319, acc: 0.8855\n",
      "epoch: 31, loss: 0.4155228971928394, acc: 0.883\n",
      "epoch: 31, loss: 0.39723696223663707, acc: 0.8985\n",
      "epoch: 31, loss: 0.3985499248696766, acc: 0.897\n",
      "epoch: 31, loss: 0.38760882123417895, acc: 0.892\n",
      "epoch: 31, loss: 0.4068473482624567, acc: 0.885\n",
      "epoch: 31, loss: 0.42455961049379026, acc: 0.88\n",
      "epoch: 31, loss: 0.38458591261542713, acc: 0.8925\n",
      "epoch: 31, loss: 0.43892597355531127, acc: 0.876\n",
      "epoch: 31, loss: 0.42296206946592463, acc: 0.8805\n",
      "epoch: 31, loss: 0.42563698801281985, acc: 0.8845\n",
      "epoch: 31, loss: 0.36889139279407707, acc: 0.898\n",
      "epoch: 31, loss: 0.4048295439271693, acc: 0.8905\n",
      "epoch: 31, loss: 0.39753426874970854, acc: 0.8915\n",
      "epoch: 31, loss: 0.4146235857306853, acc: 0.88\n",
      "epoch: 31, loss: 0.4241293308099498, acc: 0.8805\n",
      "epoch: 31, loss: 0.3824554010271289, acc: 0.892\n",
      "epoch: 31, loss: 0.3937918224887243, acc: 0.894\n",
      "epoch: 31, loss: 0.38587605346130016, acc: 0.8925\n",
      "epoch: 31, loss: 0.3896487081218595, acc: 0.885\n",
      "epoch: 31, loss: 0.42506510518561585, acc: 0.883\n",
      "epoch: 31, loss: 0.4057411311527161, acc: 0.886\n",
      "epoch: 31, loss: 0.4067345267835801, acc: 0.8845\n",
      "epoch: 31, loss: 0.3950514429748318, acc: 0.891\n",
      "n_batches 24\n",
      "epoch: 32, loss: 0.4079503164150858, acc: 0.887\n",
      "epoch: 32, loss: 0.409955991520289, acc: 0.8845\n",
      "epoch: 32, loss: 0.391400011500024, acc: 0.8995\n",
      "epoch: 32, loss: 0.3930428640479255, acc: 0.899\n",
      "epoch: 32, loss: 0.3819907590686166, acc: 0.894\n",
      "epoch: 32, loss: 0.40127494335097724, acc: 0.8875\n",
      "epoch: 32, loss: 0.4186875467542789, acc: 0.8805\n",
      "epoch: 32, loss: 0.3788645445552639, acc: 0.893\n",
      "epoch: 32, loss: 0.4334376596718322, acc: 0.8765\n",
      "epoch: 32, loss: 0.417827862354643, acc: 0.8815\n",
      "epoch: 32, loss: 0.4203593860423264, acc: 0.8855\n",
      "epoch: 32, loss: 0.3634570955089221, acc: 0.8985\n",
      "epoch: 32, loss: 0.39875898058060283, acc: 0.8905\n",
      "epoch: 32, loss: 0.3921401898292734, acc: 0.8925\n",
      "epoch: 32, loss: 0.409093748680766, acc: 0.881\n",
      "epoch: 32, loss: 0.4184356273327537, acc: 0.8825\n",
      "epoch: 32, loss: 0.3770989699917114, acc: 0.8925\n",
      "epoch: 32, loss: 0.3882721692423256, acc: 0.896\n",
      "epoch: 32, loss: 0.3802607801742739, acc: 0.893\n",
      "epoch: 32, loss: 0.38419052092690514, acc: 0.886\n",
      "epoch: 32, loss: 0.4198405983641205, acc: 0.884\n",
      "epoch: 32, loss: 0.4004748469364668, acc: 0.886\n",
      "epoch: 32, loss: 0.401480064922755, acc: 0.8865\n",
      "epoch: 32, loss: 0.389642722260573, acc: 0.8915\n",
      "n_batches 24\n",
      "epoch: 33, loss: 0.40241832118386156, acc: 0.8895\n",
      "epoch: 33, loss: 0.4047456987137851, acc: 0.8855\n",
      "epoch: 33, loss: 0.38593096897605567, acc: 0.9005\n",
      "epoch: 33, loss: 0.38790093668107684, acc: 0.8995\n",
      "epoch: 33, loss: 0.37674093105118334, acc: 0.896\n",
      "epoch: 33, loss: 0.39605715206932235, acc: 0.8885\n",
      "epoch: 33, loss: 0.41318567951066687, acc: 0.882\n",
      "epoch: 33, loss: 0.37351381779793075, acc: 0.895\n",
      "epoch: 33, loss: 0.4282826336591438, acc: 0.8775\n",
      "epoch: 33, loss: 0.41304067061980254, acc: 0.8825\n",
      "epoch: 33, loss: 0.41539933961757836, acc: 0.8885\n",
      "epoch: 33, loss: 0.358365322561293, acc: 0.9005\n",
      "epoch: 33, loss: 0.3930628701177666, acc: 0.892\n",
      "epoch: 33, loss: 0.3871077366319472, acc: 0.894\n",
      "epoch: 33, loss: 0.40391109862353036, acc: 0.8815\n",
      "epoch: 33, loss: 0.4130859341021771, acc: 0.8835\n",
      "epoch: 33, loss: 0.3720951613570052, acc: 0.893\n",
      "epoch: 33, loss: 0.38310673443372595, acc: 0.8975\n",
      "epoch: 33, loss: 0.3749682719435905, acc: 0.8945\n",
      "epoch: 33, loss: 0.3790651262039619, acc: 0.887\n",
      "epoch: 33, loss: 0.4149369225868743, acc: 0.884\n",
      "epoch: 33, loss: 0.3955631209045092, acc: 0.887\n",
      "epoch: 33, loss: 0.3965720748921147, acc: 0.888\n",
      "epoch: 33, loss: 0.38458324538222977, acc: 0.8915\n",
      "n_batches 24\n",
      "epoch: 34, loss: 0.39723532404091433, acc: 0.891\n",
      "epoch: 34, loss: 0.39985437528411305, acc: 0.886\n",
      "epoch: 34, loss: 0.38080308159132087, acc: 0.902\n",
      "epoch: 34, loss: 0.38308838166700815, acc: 0.901\n",
      "epoch: 34, loss: 0.3718311353450704, acc: 0.898\n",
      "epoch: 34, loss: 0.39116353911308027, acc: 0.8905\n",
      "epoch: 34, loss: 0.40803028581834155, acc: 0.8845\n",
      "epoch: 34, loss: 0.36848736045979563, acc: 0.898\n",
      "epoch: 34, loss: 0.4234391204225462, acc: 0.8785\n",
      "epoch: 34, loss: 0.4085498840003993, acc: 0.8835\n",
      "epoch: 34, loss: 0.41073129726080526, acc: 0.8885\n",
      "epoch: 34, loss: 0.3535858864981085, acc: 0.901\n",
      "epoch: 34, loss: 0.3877166682922693, acc: 0.8925\n",
      "epoch: 34, loss: 0.3823913140937018, acc: 0.897\n",
      "epoch: 34, loss: 0.3990424680760672, acc: 0.884\n",
      "epoch: 34, loss: 0.4080545429417763, acc: 0.884\n",
      "epoch: 34, loss: 0.3674065142147943, acc: 0.8925\n",
      "epoch: 34, loss: 0.3782666527150868, acc: 0.8995\n",
      "epoch: 34, loss: 0.36998233249031987, acc: 0.8955\n",
      "epoch: 34, loss: 0.37425306856256774, acc: 0.8895\n",
      "epoch: 34, loss: 0.4103220440649848, acc: 0.8845\n",
      "epoch: 34, loss: 0.3909749650733457, acc: 0.89\n",
      "epoch: 34, loss: 0.39197366684594465, acc: 0.8875\n",
      "epoch: 34, loss: 0.37984489679491196, acc: 0.892\n",
      "n_batches 24\n",
      "epoch: 35, loss: 0.39235583363370496, acc: 0.8925\n",
      "epoch: 35, loss: 0.395265327644319, acc: 0.889\n",
      "epoch: 35, loss: 0.3759745360330037, acc: 0.903\n",
      "epoch: 35, loss: 0.37857706582151013, acc: 0.9045\n",
      "epoch: 35, loss: 0.36721609584398035, acc: 0.9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 35, loss: 0.386553295713692, acc: 0.8915\n",
      "epoch: 35, loss: 0.40319854233063257, acc: 0.884\n",
      "epoch: 35, loss: 0.3637644322501847, acc: 0.8995\n",
      "epoch: 35, loss: 0.41888419210717853, acc: 0.8795\n",
      "epoch: 35, loss: 0.40434330477272706, acc: 0.885\n",
      "epoch: 35, loss: 0.4063301118096119, acc: 0.8905\n",
      "epoch: 35, loss: 0.3490907337078111, acc: 0.9015\n",
      "epoch: 35, loss: 0.3826829173353445, acc: 0.8945\n",
      "epoch: 35, loss: 0.3779737726181498, acc: 0.896\n",
      "epoch: 35, loss: 0.3944603712803589, acc: 0.8855\n",
      "epoch: 35, loss: 0.4033246628454809, acc: 0.8855\n",
      "epoch: 35, loss: 0.36300818291860154, acc: 0.8935\n",
      "epoch: 35, loss: 0.3737181084798081, acc: 0.9005\n",
      "epoch: 35, loss: 0.3652835772656965, acc: 0.8965\n",
      "epoch: 35, loss: 0.36972998582943406, acc: 0.891\n",
      "epoch: 35, loss: 0.4059730531505791, acc: 0.8855\n",
      "epoch: 35, loss: 0.3866708006711367, acc: 0.8915\n",
      "epoch: 35, loss: 0.387661611179124, acc: 0.8875\n",
      "epoch: 35, loss: 0.37539915142920727, acc: 0.8935\n",
      "n_batches 24\n",
      "epoch: 36, loss: 0.38776986836809535, acc: 0.8935\n",
      "epoch: 36, loss: 0.39093941954255557, acc: 0.889\n",
      "epoch: 36, loss: 0.371424077252548, acc: 0.904\n",
      "epoch: 36, loss: 0.3743264130725691, acc: 0.907\n",
      "epoch: 36, loss: 0.36288433322115293, acc: 0.901\n",
      "epoch: 36, loss: 0.38220957650032344, acc: 0.8945\n",
      "epoch: 36, loss: 0.39864168035593306, acc: 0.8845\n",
      "epoch: 36, loss: 0.3593170482041349, acc: 0.9005\n",
      "epoch: 36, loss: 0.4145919785245882, acc: 0.8805\n",
      "epoch: 36, loss: 0.4003860778160253, acc: 0.885\n",
      "epoch: 36, loss: 0.4021665565300735, acc: 0.892\n",
      "epoch: 36, loss: 0.3448516322652015, acc: 0.9035\n",
      "epoch: 36, loss: 0.3779512631037097, acc: 0.896\n",
      "epoch: 36, loss: 0.3738191675961478, acc: 0.8975\n",
      "epoch: 36, loss: 0.39014386728132316, acc: 0.8895\n",
      "epoch: 36, loss: 0.3988618162937693, acc: 0.887\n",
      "epoch: 36, loss: 0.35886901497768564, acc: 0.895\n",
      "epoch: 36, loss: 0.36945079317565593, acc: 0.901\n",
      "epoch: 36, loss: 0.3608419372478147, acc: 0.8975\n",
      "epoch: 36, loss: 0.3654734489022411, acc: 0.8935\n",
      "epoch: 36, loss: 0.4018601146324171, acc: 0.887\n",
      "epoch: 36, loss: 0.3826330534157662, acc: 0.8965\n",
      "epoch: 36, loss: 0.3836022020084301, acc: 0.889\n",
      "epoch: 36, loss: 0.37120954992337324, acc: 0.894\n",
      "n_batches 24\n",
      "epoch: 37, loss: 0.38343593462537434, acc: 0.8935\n",
      "epoch: 37, loss: 0.38685609394878784, acc: 0.8905\n",
      "epoch: 37, loss: 0.367129326693787, acc: 0.9045\n",
      "epoch: 37, loss: 0.3703259261210581, acc: 0.908\n",
      "epoch: 37, loss: 0.35879846898488754, acc: 0.903\n",
      "epoch: 37, loss: 0.37810642274103984, acc: 0.896\n",
      "epoch: 37, loss: 0.394343134282018, acc: 0.888\n",
      "epoch: 37, loss: 0.35512786114605727, acc: 0.9005\n",
      "epoch: 37, loss: 0.41052720123445746, acc: 0.8825\n",
      "epoch: 37, loss: 0.39664497044345093, acc: 0.886\n",
      "epoch: 37, loss: 0.39822696751329717, acc: 0.893\n",
      "epoch: 37, loss: 0.3408587217029535, acc: 0.905\n",
      "epoch: 37, loss: 0.3734931115546951, acc: 0.897\n",
      "epoch: 37, loss: 0.36991858607262595, acc: 0.8985\n",
      "epoch: 37, loss: 0.38606600490204424, acc: 0.8895\n",
      "epoch: 37, loss: 0.39464112923282746, acc: 0.888\n",
      "epoch: 37, loss: 0.35497737521863654, acc: 0.896\n",
      "epoch: 37, loss: 0.365426101124175, acc: 0.9045\n",
      "epoch: 37, loss: 0.35664146544802483, acc: 0.898\n",
      "epoch: 37, loss: 0.36145850790205997, acc: 0.8945\n",
      "epoch: 37, loss: 0.3979630239373505, acc: 0.887\n",
      "epoch: 37, loss: 0.3788297992601783, acc: 0.8975\n",
      "epoch: 37, loss: 0.37977913625185045, acc: 0.8915\n",
      "epoch: 37, loss: 0.3672603239130748, acc: 0.8945\n",
      "n_batches 24\n",
      "epoch: 38, loss: 0.3793373264626332, acc: 0.894\n",
      "epoch: 38, loss: 0.3829850868085668, acc: 0.891\n",
      "epoch: 38, loss: 0.36306177192989786, acc: 0.9055\n",
      "epoch: 38, loss: 0.3665412012678247, acc: 0.909\n",
      "epoch: 38, loss: 0.3549404532337267, acc: 0.9045\n",
      "epoch: 38, loss: 0.37422271449040967, acc: 0.896\n",
      "epoch: 38, loss: 0.3902878726644721, acc: 0.8895\n",
      "epoch: 38, loss: 0.3511633873666733, acc: 0.901\n",
      "epoch: 38, loss: 0.4066862324873636, acc: 0.885\n",
      "epoch: 38, loss: 0.39310829984020246, acc: 0.8875\n",
      "epoch: 38, loss: 0.39448671851515665, acc: 0.893\n",
      "epoch: 38, loss: 0.33708725065471246, acc: 0.9055\n",
      "epoch: 38, loss: 0.3692815936722611, acc: 0.8975\n",
      "epoch: 38, loss: 0.3662304293099535, acc: 0.8995\n",
      "epoch: 38, loss: 0.38220931119226426, acc: 0.8895\n",
      "epoch: 38, loss: 0.39064494639713393, acc: 0.8895\n",
      "epoch: 38, loss: 0.351300116499944, acc: 0.897\n",
      "epoch: 38, loss: 0.36162656880630756, acc: 0.906\n",
      "epoch: 38, loss: 0.3526553943629707, acc: 0.9005\n",
      "epoch: 38, loss: 0.3576448728891055, acc: 0.8955\n",
      "epoch: 38, loss: 0.3942691197891137, acc: 0.887\n",
      "epoch: 38, loss: 0.37524989661757024, acc: 0.8975\n",
      "epoch: 38, loss: 0.3761604285965107, acc: 0.8925\n",
      "epoch: 38, loss: 0.3635400556861699, acc: 0.8955\n",
      "n_batches 24\n",
      "epoch: 39, loss: 0.37545048745468407, acc: 0.8935\n",
      "epoch: 39, loss: 0.379315363313026, acc: 0.891\n",
      "epoch: 39, loss: 0.35921618116175147, acc: 0.9055\n",
      "epoch: 39, loss: 0.36295472006488344, acc: 0.909\n",
      "epoch: 39, loss: 0.3512870469706488, acc: 0.905\n",
      "epoch: 39, loss: 0.3705477756372637, acc: 0.8955\n",
      "epoch: 39, loss: 0.38644790423074027, acc: 0.8905\n",
      "epoch: 39, loss: 0.3474117704354099, acc: 0.901\n",
      "epoch: 39, loss: 0.40303820570307836, acc: 0.8855\n",
      "epoch: 39, loss: 0.389762405910906, acc: 0.888\n",
      "epoch: 39, loss: 0.3909207720022604, acc: 0.894\n",
      "epoch: 39, loss: 0.3335196040299403, acc: 0.907\n",
      "epoch: 39, loss: 0.36529599919402356, acc: 0.9015\n",
      "epoch: 39, loss: 0.3627406848478349, acc: 0.8995\n",
      "epoch: 39, loss: 0.378547835878786, acc: 0.8905\n",
      "epoch: 39, loss: 0.3868550273432183, acc: 0.89\n",
      "epoch: 39, loss: 0.3478259133240241, acc: 0.8975\n",
      "epoch: 39, loss: 0.35802653714132804, acc: 0.9065\n",
      "epoch: 39, loss: 0.3488722475048361, acc: 0.903\n",
      "epoch: 39, loss: 0.3540287926748909, acc: 0.897\n",
      "epoch: 39, loss: 0.3907643725109502, acc: 0.8875\n",
      "epoch: 39, loss: 0.3718563765804336, acc: 0.898\n",
      "epoch: 39, loss: 0.3727366532281012, acc: 0.894\n",
      "epoch: 39, loss: 0.36002507894234587, acc: 0.896\n",
      "n_batches 24\n",
      "epoch: 40, loss: 0.3717641829131421, acc: 0.894\n",
      "epoch: 40, loss: 0.3758288429080838, acc: 0.8925\n",
      "epoch: 40, loss: 0.3555633986937459, acc: 0.906\n",
      "epoch: 40, loss: 0.35955091030171643, acc: 0.911\n",
      "epoch: 40, loss: 0.34781724565235655, acc: 0.905\n",
      "epoch: 40, loss: 0.36706012999007814, acc: 0.896\n",
      "epoch: 40, loss: 0.38280062765815764, acc: 0.8935\n",
      "epoch: 40, loss: 0.3438528681575434, acc: 0.9035\n",
      "epoch: 40, loss: 0.3995776894637331, acc: 0.8865\n",
      "epoch: 40, loss: 0.386591385211811, acc: 0.888\n",
      "epoch: 40, loss: 0.38751838374212005, acc: 0.8945\n",
      "epoch: 40, loss: 0.3301365946915405, acc: 0.908\n",
      "epoch: 40, loss: 0.36150604568202593, acc: 0.9025\n",
      "epoch: 40, loss: 0.3594368339436991, acc: 0.9005\n",
      "epoch: 40, loss: 0.37506695700269443, acc: 0.8905\n",
      "epoch: 40, loss: 0.3832434752626375, acc: 0.8935\n",
      "epoch: 40, loss: 0.34453629003642905, acc: 0.8995\n",
      "epoch: 40, loss: 0.3546076474569106, acc: 0.907\n",
      "epoch: 40, loss: 0.3452707990941996, acc: 0.9035\n",
      "epoch: 40, loss: 0.35058902184301494, acc: 0.8975\n",
      "epoch: 40, loss: 0.38742609040032994, acc: 0.887\n",
      "epoch: 40, loss: 0.3686531934145695, acc: 0.899\n",
      "epoch: 40, loss: 0.36948833900267625, acc: 0.8945\n",
      "epoch: 40, loss: 0.3566849744612184, acc: 0.8985\n",
      "n_batches 24\n",
      "epoch: 41, loss: 0.36826690290708414, acc: 0.8945\n",
      "epoch: 41, loss: 0.3725062086769536, acc: 0.8935\n",
      "epoch: 41, loss: 0.35208847124659054, acc: 0.906\n",
      "epoch: 41, loss: 0.35631509632766184, acc: 0.9115\n",
      "epoch: 41, loss: 0.34452203149692395, acc: 0.905\n",
      "epoch: 41, loss: 0.3637371450361103, acc: 0.897\n",
      "epoch: 41, loss: 0.3793315130078126, acc: 0.8945\n",
      "epoch: 41, loss: 0.3404668809436166, acc: 0.904\n",
      "epoch: 41, loss: 0.3962789406840074, acc: 0.8875\n",
      "epoch: 41, loss: 0.3835805863483248, acc: 0.8905\n",
      "epoch: 41, loss: 0.38427357959580755, acc: 0.8945\n",
      "epoch: 41, loss: 0.32692141831561716, acc: 0.908\n",
      "epoch: 41, loss: 0.3579072086801742, acc: 0.9035\n",
      "epoch: 41, loss: 0.3563041390857619, acc: 0.901\n",
      "epoch: 41, loss: 0.371749029544723, acc: 0.8905\n",
      "epoch: 41, loss: 0.3798062741648154, acc: 0.8935\n",
      "epoch: 41, loss: 0.3414260754481934, acc: 0.901\n",
      "epoch: 41, loss: 0.3513529667431261, acc: 0.9075\n",
      "epoch: 41, loss: 0.3418344418928403, acc: 0.904\n",
      "epoch: 41, loss: 0.3473106964547745, acc: 0.899\n",
      "epoch: 41, loss: 0.3842405677026482, acc: 0.8875\n",
      "epoch: 41, loss: 0.3656092487184052, acc: 0.899\n",
      "epoch: 41, loss: 0.3663976368904143, acc: 0.896\n",
      "epoch: 41, loss: 0.3535088166624107, acc: 0.899\n",
      "n_batches 24\n",
      "epoch: 42, loss: 0.36492475596014, acc: 0.895\n",
      "epoch: 42, loss: 0.3693437043052431, acc: 0.8935\n",
      "epoch: 42, loss: 0.34878112684478696, acc: 0.9065\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 42, loss: 0.35323662604668393, acc: 0.9135\n",
      "epoch: 42, loss: 0.3414020390675061, acc: 0.9055\n",
      "epoch: 42, loss: 0.3605709849233976, acc: 0.8975\n",
      "epoch: 42, loss: 0.3760304460427562, acc: 0.8955\n",
      "epoch: 42, loss: 0.3372421820879764, acc: 0.905\n",
      "epoch: 42, loss: 0.39313567865152377, acc: 0.889\n",
      "epoch: 42, loss: 0.3807095909153536, acc: 0.8915\n",
      "epoch: 42, loss: 0.3811763792200119, acc: 0.8965\n",
      "epoch: 42, loss: 0.32385367800499093, acc: 0.909\n",
      "epoch: 42, loss: 0.3544818397090421, acc: 0.906\n",
      "epoch: 42, loss: 0.353323890250088, acc: 0.902\n",
      "epoch: 42, loss: 0.36859481613463924, acc: 0.8905\n",
      "epoch: 42, loss: 0.3765236232683459, acc: 0.8935\n",
      "epoch: 42, loss: 0.33847373950664167, acc: 0.901\n",
      "epoch: 42, loss: 0.3482595028934388, acc: 0.908\n",
      "epoch: 42, loss: 0.3385596089930576, acc: 0.9045\n",
      "epoch: 42, loss: 0.3441949470761413, acc: 0.9\n",
      "epoch: 42, loss: 0.3811975248597654, acc: 0.8885\n",
      "epoch: 42, loss: 0.3627156216216927, acc: 0.9\n",
      "epoch: 42, loss: 0.36345236579043877, acc: 0.8965\n",
      "epoch: 42, loss: 0.35050508209617554, acc: 0.8995\n",
      "n_batches 24\n",
      "epoch: 43, loss: 0.3617379862737551, acc: 0.8955\n",
      "epoch: 43, loss: 0.3663206780553482, acc: 0.894\n",
      "epoch: 43, loss: 0.3456265282162193, acc: 0.907\n",
      "epoch: 43, loss: 0.3502965056195036, acc: 0.9135\n",
      "epoch: 43, loss: 0.33842561393559195, acc: 0.9065\n",
      "epoch: 43, loss: 0.3575468872752163, acc: 0.8965\n",
      "epoch: 43, loss: 0.37288526209633915, acc: 0.8965\n",
      "epoch: 43, loss: 0.33416318911831594, acc: 0.906\n",
      "epoch: 43, loss: 0.3901420914320848, acc: 0.892\n",
      "epoch: 43, loss: 0.3779694903551916, acc: 0.8915\n",
      "epoch: 43, loss: 0.3782137721837884, acc: 0.8965\n",
      "epoch: 43, loss: 0.32093121356100446, acc: 0.91\n",
      "epoch: 43, loss: 0.35121937704124256, acc: 0.907\n",
      "epoch: 43, loss: 0.35048574326595733, acc: 0.902\n",
      "epoch: 43, loss: 0.3655825857575934, acc: 0.891\n",
      "epoch: 43, loss: 0.3733876780769854, acc: 0.8945\n",
      "epoch: 43, loss: 0.3356644469685193, acc: 0.9015\n",
      "epoch: 43, loss: 0.3453264824182427, acc: 0.9085\n",
      "epoch: 43, loss: 0.33541956318971944, acc: 0.9055\n",
      "epoch: 43, loss: 0.3412140045921634, acc: 0.901\n",
      "epoch: 43, loss: 0.3782887162395175, acc: 0.891\n",
      "epoch: 43, loss: 0.3599497117773059, acc: 0.9\n",
      "epoch: 43, loss: 0.3606362108791331, acc: 0.897\n",
      "epoch: 43, loss: 0.34764727633727477, acc: 0.9015\n",
      "n_batches 24\n",
      "epoch: 44, loss: 0.3586924643795056, acc: 0.8965\n",
      "epoch: 44, loss: 0.36342838994995613, acc: 0.8945\n",
      "epoch: 44, loss: 0.34261203012192326, acc: 0.9075\n",
      "epoch: 44, loss: 0.3474956497220503, acc: 0.913\n",
      "epoch: 44, loss: 0.3355808342104112, acc: 0.9075\n",
      "epoch: 44, loss: 0.35466365003379813, acc: 0.897\n",
      "epoch: 44, loss: 0.3698808941828771, acc: 0.898\n",
      "epoch: 44, loss: 0.3312274789206767, acc: 0.9075\n",
      "epoch: 44, loss: 0.38728595437479896, acc: 0.8925\n",
      "epoch: 44, loss: 0.3753464802132901, acc: 0.8925\n",
      "epoch: 44, loss: 0.375374326269217, acc: 0.897\n",
      "epoch: 44, loss: 0.31814349311400597, acc: 0.9105\n",
      "epoch: 44, loss: 0.348108360434978, acc: 0.9075\n",
      "epoch: 44, loss: 0.34777756161771584, acc: 0.903\n",
      "epoch: 44, loss: 0.3626978224536445, acc: 0.893\n",
      "epoch: 44, loss: 0.370382392171795, acc: 0.8955\n",
      "epoch: 44, loss: 0.33299052764291104, acc: 0.902\n",
      "epoch: 44, loss: 0.34251618234174425, acc: 0.909\n",
      "epoch: 44, loss: 0.33241649402777296, acc: 0.906\n",
      "epoch: 44, loss: 0.3383603140927505, acc: 0.9015\n",
      "epoch: 44, loss: 0.37550290462728253, acc: 0.8915\n",
      "epoch: 44, loss: 0.3573152713334234, acc: 0.901\n",
      "epoch: 44, loss: 0.3579357429533743, acc: 0.8975\n",
      "epoch: 44, loss: 0.3449241204127345, acc: 0.9035\n",
      "n_batches 24\n",
      "epoch: 45, loss: 0.3557738871791386, acc: 0.8975\n",
      "epoch: 45, loss: 0.36065234097475957, acc: 0.8955\n",
      "epoch: 45, loss: 0.33973070232100394, acc: 0.907\n",
      "epoch: 45, loss: 0.3448118749148147, acc: 0.9135\n",
      "epoch: 45, loss: 0.332877830788402, acc: 0.9075\n",
      "epoch: 45, loss: 0.3519040330691155, acc: 0.898\n",
      "epoch: 45, loss: 0.36700853795588634, acc: 0.8995\n",
      "epoch: 45, loss: 0.32841514253449533, acc: 0.9085\n",
      "epoch: 45, loss: 0.3845543250952099, acc: 0.893\n",
      "epoch: 45, loss: 0.37283364424025617, acc: 0.8925\n",
      "epoch: 45, loss: 0.3726464914377074, acc: 0.898\n",
      "epoch: 45, loss: 0.31546978985380175, acc: 0.9105\n",
      "epoch: 45, loss: 0.34513739022612033, acc: 0.908\n",
      "epoch: 45, loss: 0.3451821623734501, acc: 0.903\n",
      "epoch: 45, loss: 0.35993546457782305, acc: 0.8935\n",
      "epoch: 45, loss: 0.3675091935511449, acc: 0.897\n",
      "epoch: 45, loss: 0.33043905174764304, acc: 0.904\n",
      "epoch: 45, loss: 0.33982047349359296, acc: 0.91\n",
      "epoch: 45, loss: 0.32953261356230634, acc: 0.907\n",
      "epoch: 45, loss: 0.335628509144891, acc: 0.9015\n",
      "epoch: 45, loss: 0.3728364490446634, acc: 0.892\n",
      "epoch: 45, loss: 0.35478972895724703, acc: 0.9015\n",
      "epoch: 45, loss: 0.35534886452664055, acc: 0.898\n",
      "epoch: 45, loss: 0.34232321370894436, acc: 0.9045\n",
      "n_batches 24\n",
      "epoch: 46, loss: 0.35298456092692515, acc: 0.8975\n",
      "epoch: 46, loss: 0.35799506980531304, acc: 0.8965\n",
      "epoch: 46, loss: 0.3369647462379653, acc: 0.907\n",
      "epoch: 46, loss: 0.3422279761212467, acc: 0.914\n",
      "epoch: 46, loss: 0.33029511272021317, acc: 0.9085\n",
      "epoch: 46, loss: 0.3492577532802735, acc: 0.899\n",
      "epoch: 46, loss: 0.36426156438572055, acc: 0.8995\n",
      "epoch: 46, loss: 0.3257232146035571, acc: 0.9095\n",
      "epoch: 46, loss: 0.3819321461936655, acc: 0.8935\n",
      "epoch: 46, loss: 0.37042362871891293, acc: 0.8925\n",
      "epoch: 46, loss: 0.37001915628716064, acc: 0.8985\n",
      "epoch: 46, loss: 0.3129032769137632, acc: 0.9115\n",
      "epoch: 46, loss: 0.34230394337950176, acc: 0.9085\n",
      "epoch: 46, loss: 0.342697644238924, acc: 0.9035\n",
      "epoch: 46, loss: 0.357286533222412, acc: 0.8945\n",
      "epoch: 46, loss: 0.364758810193479, acc: 0.8985\n",
      "epoch: 46, loss: 0.32799049851436496, acc: 0.905\n",
      "epoch: 46, loss: 0.3372395662605541, acc: 0.91\n",
      "epoch: 46, loss: 0.3267623856536022, acc: 0.9085\n",
      "epoch: 46, loss: 0.33300798740903836, acc: 0.9015\n",
      "epoch: 46, loss: 0.37026891518745936, acc: 0.893\n",
      "epoch: 46, loss: 0.35238572532891543, acc: 0.9015\n",
      "epoch: 46, loss: 0.3528598430803517, acc: 0.898\n",
      "epoch: 46, loss: 0.3398292043389345, acc: 0.9055\n",
      "n_batches 24\n",
      "epoch: 47, loss: 0.3502923092333852, acc: 0.898\n",
      "epoch: 47, loss: 0.35542891444387104, acc: 0.8985\n",
      "epoch: 47, loss: 0.33431554425258025, acc: 0.908\n",
      "epoch: 47, loss: 0.3397536953060845, acc: 0.914\n",
      "epoch: 47, loss: 0.32782040394167605, acc: 0.9085\n",
      "epoch: 47, loss: 0.3467160666142762, acc: 0.899\n",
      "epoch: 47, loss: 0.3616234279942912, acc: 0.9005\n",
      "epoch: 47, loss: 0.32315171786282615, acc: 0.9105\n",
      "epoch: 47, loss: 0.37940583023700697, acc: 0.8955\n",
      "epoch: 47, loss: 0.3680980062880641, acc: 0.893\n",
      "epoch: 47, loss: 0.36749358500336066, acc: 0.9\n",
      "epoch: 47, loss: 0.3104342365365769, acc: 0.914\n",
      "epoch: 47, loss: 0.3395868255206393, acc: 0.909\n",
      "epoch: 47, loss: 0.34032738876503926, acc: 0.904\n",
      "epoch: 47, loss: 0.35473570272383403, acc: 0.895\n",
      "epoch: 47, loss: 0.36211663794258336, acc: 0.8995\n",
      "epoch: 47, loss: 0.3256377148674392, acc: 0.9055\n",
      "epoch: 47, loss: 0.3347765233913245, acc: 0.911\n",
      "epoch: 47, loss: 0.32409920565059336, acc: 0.909\n",
      "epoch: 47, loss: 0.33049165452960155, acc: 0.9015\n",
      "epoch: 47, loss: 0.36778884877618245, acc: 0.8935\n",
      "epoch: 47, loss: 0.35008053586936283, acc: 0.902\n",
      "epoch: 47, loss: 0.35046508005052324, acc: 0.898\n",
      "epoch: 47, loss: 0.33743549426249636, acc: 0.9065\n",
      "n_batches 24\n",
      "epoch: 48, loss: 0.34770768456395107, acc: 0.8985\n",
      "epoch: 48, loss: 0.3529566159108516, acc: 0.8995\n",
      "epoch: 48, loss: 0.33176942873385246, acc: 0.9085\n",
      "epoch: 48, loss: 0.33737812008707635, acc: 0.915\n",
      "epoch: 48, loss: 0.32543579497465835, acc: 0.9085\n",
      "epoch: 48, loss: 0.34427167240968526, acc: 0.9005\n",
      "epoch: 48, loss: 0.3590943685386959, acc: 0.9005\n",
      "epoch: 48, loss: 0.32068284108924283, acc: 0.9125\n",
      "epoch: 48, loss: 0.3769762947617404, acc: 0.896\n",
      "epoch: 48, loss: 0.36585697480770146, acc: 0.8935\n",
      "epoch: 48, loss: 0.36505614701607175, acc: 0.8995\n",
      "epoch: 48, loss: 0.30806105676095236, acc: 0.914\n",
      "epoch: 48, loss: 0.336986822349197, acc: 0.909\n",
      "epoch: 48, loss: 0.33805336153196386, acc: 0.905\n",
      "epoch: 48, loss: 0.3522887383539347, acc: 0.8955\n",
      "epoch: 48, loss: 0.3595793903329441, acc: 0.8995\n",
      "epoch: 48, loss: 0.3233880317564114, acc: 0.906\n",
      "epoch: 48, loss: 0.3324082017028266, acc: 0.9105\n",
      "epoch: 48, loss: 0.32153024731043117, acc: 0.9095\n",
      "epoch: 48, loss: 0.3280723295825572, acc: 0.902\n",
      "epoch: 48, loss: 0.36540011754164914, acc: 0.895\n",
      "epoch: 48, loss: 0.3478626455617865, acc: 0.9035\n",
      "epoch: 48, loss: 0.34816876350048914, acc: 0.8985\n",
      "epoch: 48, loss: 0.33513487115026713, acc: 0.9075\n",
      "n_batches 24\n",
      "epoch: 49, loss: 0.345218718460029, acc: 0.898\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 49, loss: 0.35057290719949336, acc: 0.8995\n",
      "epoch: 49, loss: 0.3293159346409898, acc: 0.909\n",
      "epoch: 49, loss: 0.33508791933704346, acc: 0.9155\n",
      "epoch: 49, loss: 0.3231570092225782, acc: 0.9085\n",
      "epoch: 49, loss: 0.34191915943884144, acc: 0.902\n",
      "epoch: 49, loss: 0.3566544431500346, acc: 0.9005\n",
      "epoch: 49, loss: 0.31831121144856606, acc: 0.912\n",
      "epoch: 49, loss: 0.37463702030049306, acc: 0.896\n",
      "epoch: 49, loss: 0.3637125024715848, acc: 0.894\n",
      "epoch: 49, loss: 0.36270217183066683, acc: 0.9\n",
      "epoch: 49, loss: 0.30577986093721743, acc: 0.914\n",
      "epoch: 49, loss: 0.3344879548399315, acc: 0.9095\n",
      "epoch: 49, loss: 0.3358788884707834, acc: 0.9045\n",
      "epoch: 49, loss: 0.34993697398896506, acc: 0.8975\n",
      "epoch: 49, loss: 0.35712932957773275, acc: 0.9\n",
      "epoch: 49, loss: 0.3212304063576119, acc: 0.906\n",
      "epoch: 49, loss: 0.33014336852832127, acc: 0.911\n",
      "epoch: 49, loss: 0.3190545083072242, acc: 0.9105\n",
      "epoch: 49, loss: 0.3257435267329304, acc: 0.902\n",
      "epoch: 49, loss: 0.36309977549636263, acc: 0.8965\n",
      "epoch: 49, loss: 0.3457297572382163, acc: 0.9035\n",
      "epoch: 49, loss: 0.34595128795555763, acc: 0.8985\n",
      "epoch: 49, loss: 0.3329283811488564, acc: 0.9075\n"
     ]
    }
   ],
   "source": [
    "nn=neural_network()\n",
    "nn.train(x_train,y_train, 2000, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(\"mnsit/nnn_mnist_scratch/mnist/mnist_train.csv\")\n",
    "x=df.iloc[:,1:]\n",
    "y=df.iloc[:,0]\n",
    "\n",
    "x_train,x_val,y_train,y_val=train_test_split(x,y,test_size=0.2,random_state=12)\n",
    "x_train=x_train.to_numpy()\n",
    "x_val=x_val.to_numpy()\n",
    "y_train=np.expand_dims(y_train.to_numpy(),1)\n",
    "y_val=np.expand_dims(y_val.to_numpy(),1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train=x_train.astype('float32')\n",
    "x_val=x_val.astype('float32')\n",
    "x_train/=255\n",
    "x_val/=255\n",
    "\n",
    "x_train,x_val, y_train, y_val=x_train.T,x_val.T, y_train.T, y_val.T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9040833333333333"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ans=nn.forward(x_val)\n",
    "\n",
    "a=[]\n",
    "for i in range(ans.shape[1]):\n",
    "    a.append(np.argmax(ans[:,i]))\n",
    "    \n",
    "accuracy_score(a,y_val.squeeze(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
